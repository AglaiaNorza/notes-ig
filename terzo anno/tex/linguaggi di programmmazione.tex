% TEX program = xelatex
\documentclass[a4paper,11pt]{report}
\usepackage[italian]{babel}

\usepackage{./../packages/mainstyle}
\usepackage{./../packages/colors}
\usepackage{./../packages/frameboxes}
\usepackage{./../packages/packs}
\usepackage{./../packages/title}
\usepackage{./../packages/macros}

\usepackage{graphicx}
\usepackage{bussproofs}
\usetikzlibrary{arrows.meta, positioning, calc}

\setcoursename{Linguaggi di Programmazione}
\setcoursebook{non usato,\\ integrati con le \href{http://wwwusers.di.uniroma1.it/~lpara/DISPENSE/note.pdf}{\underline{dispense}} del professor Cenciarelli}
\setauthorname{aglaia norza}
\setauthoremail{thisisaglaia@gmail.com}
\setauthorgithub{AglaiaNorza}

\lstdefinelanguage{SML}{
  morekeywords={fun, val, if, then, else, let, in, end, fn, datatype, case, of},
  sensitive=true,
  morecomment=[n]{(*}{*)},
  morestring=[b]"
}

\lstset{
  language=SML,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{DeepRed}\bfseries,
  commentstyle=\color{gray},
  stringstyle=\color{teal},
  showstringspaces=false,
  frame=single,
  columns=fullflexible,
  backgroundcolor=\color{gray!5},
}

\newcommand{\farrow}{\overset{fin}{\rightharpoonup}}
\newcommand{\brule}{\overset{\beta}{\longrightarrow}}

\begin{document}

\maketitle

\tableofcontents

\chapter{Algebre induttive}

\section{I numeri naturali}

\begin{defframe}{Assiomi di Peano}{}
    L'insieme \(\mathbb{N}\) dei numeri naturali si può definire mediante i cinque \textbf{assiomi di Peano}:
    \begin{enumerate}[label=\arabic*)]
        \item \( 0 \in \mathbb{N} \)
        \item \( n \in  \mathbb{N} \implies \text{succ}(n) \in \mathbb{N} \)
        \item \( \not\exists n \in \mathbb{N} \mid 0 = \text{succ}(n) \)
        \item \( \forall n, m \ \text{succ}(n) = \text{succ}(m) \implies n = m \) \ \  {\color{gray}\small (iniettività)}
        \item \( \forall S \subseteq \mathbb{N} \ \ {\color{gray}(}0 \in S \land (n \in S \implies \text{succ}(n)\in S) \implies S = \mathbb{N}{\color{gray})}\) \ \ {\color{gray}\small (assioma di induzione)}

    \end{enumerate} 

    \begin{gframe}{assioma di induzione}
        L'assioma di induzione è necessario per evitare di equiparare ai numeri naturali insiemi che, essenzialmente, contengono una struttura come quella di \( \mathbb{N} \), e un ``qualcosa in più''. (Se all'interno dell'insieme \( A \) che stiamo considerando esiste un altro sottoinsieme proprio che rispetta gli altri assiomi, \( A \) non rispetterà il quinto assioma di Peano). 

        In più, il quinto assioma di Peano ci fornisce essenzialmente una definizione insiemistica di induzione.
    \end{gframe}

\end{defframe}


\begin{defframe}{Principio di Induzione}{}
    L'induzione può essere definita, basandosi sulle ``proprietà'' invece che 
    sull' insiemistica, come segue:

    \[
        \forall P \ \ \ \frac{P(0), \quad P(n) \implies P(n+1)}{\forall n\; P(n)}
    \]

    (la notazione equivale a \( P(0) \land P(n) \land (P(0) \land (P(n) \implies P(n+1)))  \implies \forall n P(n) \) )

\end{defframe}

Possiamo dimostrare che il quinto assioma di Peano è equivalente al principio di induzione (in quanto i concetti di ``proprietà'' e ``sottoinsieme'' sono equivalenti).

Infatti, ad ogni proprietà corrisponde un sottoinsieme i cui elementi sono esattamente quelli che soddisfano tale proprietà

Prendiamo quindi \( S = \{n \in \mathbb{N} \mid P(n) \text{ è vera}\}\).

In questo modo, dire \( P(0) \) equivale a dire \( 0 \in S \), e dire \( P(n) \implies P(n+1) \) equivale a dire \( n \in S \implies n+1 \in S \). E, allo stesso modo, dire \( \forall n P(n) \) equivale a dire \( \forall n, n \in S\), ovvero \(S = \mathbb{N} \).


\begin{defframe}{Numeri di von Neumann}{}
    Un altro modo di descrivere i numeri naturali viene dal matematico \textbf{John von Neumann}, che definisce i numeri naturali (``numeri di von Neumann'', \( \mathcal{N} \)) in questo modo:

    \begin{itemize}
        \item \(  0_\mathcal{N} = \emptyset\)  \ \ {\color{gray}(ovvero \( \{ \} \))}

        \item \( 1_\mathcal{N} = \{ 0_\mathcal{N}\}\) \ \ {\color{gray}(ovvero \( \{ \{\}\} \))}
        \item \( 2_\mathcal{N} = \{ 0_\mathcal{N}, 1_\mathcal{N}\}\) \ \ {\color{gray}(ovvero \( \{ \ \{\}, \{\{\}\} \ \}  \))}
        \item ...

    \end{itemize}

    I numeri di von Neumann rispettano gli assiomi di peano! (dalle dispense)

\end{defframe}

\section{Algebre, algebre induttive}

\begin{gframe}{insieme unità e funzione nullaria}
    Ci è utile definire l'\textbf{insieme unità} \( \mathbb{1} = \{*\} \). \( \mathbb{1} \) è un insieme formato da un solo elemento (non ci interessa quale). 

    Un altro concetto che ci servirà è quello di \textbf{funzione costante} o \textbf{nullaria}. Una funzione nullaria \( f \) è tale che:
    \[
        f: \mathbb{1} \to A \mid f() = a \ \ \ \ a\in A
    \]
    (chiaramente, essa è sempre iniettiva).

    \begin{gframe}{}

        Una funzione nullaria su un insieme \( A \) può essere vista come un elemento di \( A \) (un qualsiasi insieme \( A \) è isomorfo a all'insieme di funzioni \( \mathbb{1}\to A \) (l'insieme di funzioni \( \mathbb{1}\to A \) ha la stessa cardinalità di \( A \)), il che ci permette di \textbf{trattare gli elementi di un insieme come funzioni}.


    \end{gframe}
\end{gframe}

\begin{defframe}{Algebra}{}
    Una \textbf{algebra} è una tupla \( (A, \Gamma) \), dove:
    \begin{itemize}
        \item \( A \) è l'insieme di riferimento (``carrier'' o ``insieme sottostante'') 
        \item \( \Gamma = \{\gamma_1, \gamma_2, \dots, \gamma_i\} \), è l'insieme di funzioni chiamate ``operazioni fondamentali'' o ``costruttori'' dell'algebra
            \subitem la segnatura dei costruttori è: \( \gamma_i:A^{\alpha_i}\times K_i \to A \).

    \end{itemize}
    \begin{gframe}{}
        Tra le algebre, consideriamo anche le algebre eterogenee, che prendono argomenti da insiemi diversi da \( A \).
    \end{gframe}
\end{defframe}

\begin{defframe}{Chiusura di un insieme rispetto ad un'operazione}{}
    Sia \( f: A^n \times K \to A \) un'operazione su \( A \) con parametri esterni \( K = (K_1 \times \dots \times K_m) \). 

    Un insieme \( S \subseteq A \) si dice \textbf{chiuso} rispetto ad \( f \) quando:
    \[
        a_1, \dots, a_n \in S \implies f( a_1, \dots, a_n,  k_1, \dots, k_n) \in S
    \]

    \begin{gframe}[colframe=RedViolet, colbacktitle=RedViolet]{nota!}
        Data un'operazione \( f \) che prende solo elementi esterni all'insieme \( S \) (come per esempio la funzione nullaria \ \( \mathbb{1} \to A \)), un insieme \( S \) si dice chiuso rispetto a \(f \iff \text{Im}(f) \subseteq S \)).

    \end{gframe}
\end{defframe}

\begin{defframe}{Algebra induttiva}{}
    Un'algebra \( A, \Gamma \) si dice \textbf{induttiva} quando:
    \begin{enumerate}
        \item tutte le \( \gamma_i \in \Gamma\) sono iniettive
        \item \( \forall i, j \mid i \neq j , \ \text{Im}(\gamma_i) \cap \text{Im}(\gamma_j) = \emptyset \), ovvero tutte le \( \gamma_i \) hanno immagini disgiunte
        \item \( \forall S\subseteq A \), se \( S \) è chiuso rispetto a tutte le \( \gamma_i \), allora \( S = A \) (ovvero il principio di induzione è rispettato)   
    \end{enumerate}
    \begin{gframe}{}
        La terza condizione pone quindi che \( A \) sia la più piccola sotto-algebra di se stessa (ovvero non abbia sotto-algebre diverse da se stessa).
    \end{gframe}       
    \begin{gframe}{}
        Le tre condizioni garantiscono quindi che:
        \begin{itemize}
            \item ci sia solo un modo per costruire ogni elemento dell'algebra (\textit{i, ii})
            \item non ci siano ``elementi inutili'' (\textit{iii})
        \end{itemize}
    \end{gframe}

\end{defframe}

Vediamo come possiamo costruire \( \mathbb{N} \) come algebra induttiva.

La definizione di algebra induttiva non considera il concetto di ``elemento'', quindi, per il primo assioma di Peano, usiamo una \textit{funzione costante} \( \mathbb{0} \), con segnatura:
\[\mathbb{1} \times \mathbb{N} : x\to 0 \]

Abbiamo quindi una tupla (\( \mathbb{N}, \{\text{succ}, \mathbb{0}\} \)).

Per dimostrare che  questa tupla sia un'algebra induttiva, dobbiamo ora verificare le tre condizioni:
\begin{enumerate}
    \item tutte le \( \gamma_i \) sono induttive:
        \subitem • \( \mathbb{0} \) è necessariamente induttiva
        \subitem • succ è induttiva per il secondo assioma di Peano
    \item tutti i costruttori hanno immagini disgiunte:
        \subitem • grazie al terzo assioma di Peano \((\not\exists n \in \mathbb{N} \mid 0 = \text{succ}(n)) \), sappiamo che succ e \( \mathbb{0} \) hanno immagini disgiunte
    \item principio di induzione:
        \subitem • è verificato dal quinto assioma di Peano (\( 0 \in S\) corrisponde alla chiusura rispetto a \(\mathbb{0} \) e \( n \in \mathbb{N} \implies \text{succ}(n) \in \mathbb{N}\) corrisponde alla chiusura rispetto a succ)
\end{enumerate}



\begin{gframe}{alberi binari come algebre induttive}
    L'insieme degli alberi binari finiti \( (\texttt{B-trees}, \text{leaf, branch}) \), dove:
    \begin{itemize}
        \item \( \texttt{B-trees}=\{t \mid t \text{ è una foglia, oppure } t=\langle t_1, t_2 \rangle \text{ con } t_1, t_2 \in \texttt{B-trees}\} \)
        \item leaf: \( 1\to \texttt{B-trees} \) \ {\color{gray}(foglia)}
        \item branch: \( \texttt{B-trees} \times  \texttt{B-trees} \to  \texttt{B-trees} : (t_{sx}, t_{dx})\to t\) \ {\color{gray}(costruisce rami in modo che \( t_{sx} \) e \( t_{dx} \) siano i due sottoalberi di \( t \))}
    \end{itemize}
    è un'algebra induttiva.
    \begin{thmframe}{numero di nodi di un albero binario}{}
        Un albero binario con \( n \) foglie ha \( 2n-1 \) nodi
        \begin{proofframe}
            Si può dimostrare per induzione strutturale sui costruttori degli alberi.
            \begin{itemize}
                \item (\textbf{caso base}): la proprietà è vera per l'albero formato da una sola foglia costruito con leaf (\( \circ \)) - esso ha infatti \( n = 1 \) foglie e \( 2n-1 = 1 \) nodi.
                \item (\textbf{ipotesi induttiva}): ogni argomento dei costruttori rispetta la proprietà
                \item dobbiamo quindi verificare che il costruttore branch, dati due argomenti che rispettano la proprietà, la mantenga
                \item (\textbf{passo induttivo}): abbiamo \( t = \text{branch}(t_1, t_2)\). 
                    \subitem Sia \(n = n_1 + n_2 \) il numero di foglie di \( t \), dove le foglie di \( t_1 \) sono \( n_1 \) e quelle di \( t_2 \) sono \( n_2 \). 

                    Per ipotesi, \( t_1 \) ha \( 2n_1 - 1 \) nodi e \( t_2 \) ne ha \( 2n_2 - 1 \). Dunque, \( t \) avrà \( (2n_1 -1) + (2n_2 -1) + 1\) nodi, ovvero \( 2(n_1 + n_2) -1 = 2n - 1 \), qed.
            \end{itemize}

        \end{proofframe}

    \end{thmframe}
\end{gframe}

\begin{gframe}{liste finite come algebra induttiva}
    \label{alist}
    Dato un insieme \( A \), indichiamo con \( A-list \) l'insieme delle liste finite di elementi di \( A \).

    La tupla \( (A-list,\text{ empty, cons}) \) è un'algebra induttiva, dove:
    \begin{itemize}
        \item empty: \( \mathbb{1} \to A-list \) è la funzione costante che restituisce la \textbf{lista vuota} ``\(\langle \rangle\)''.
        \item cons: \( A \times A-list \to A-list : \text{cons}(3, \langle 5, 7 \rangle) = \langle 3, 5, 7 \rangle \) è la funzione che \textbf{costruisce una lista} aggiungendo un elemento in testa
    \end{itemize}

    Si tratta di un'algebra induttiva (notiamo che i due costruttori hanno immagini chiaramente disgiunte, sono entrambi chiusi per \( A-list \), e c'è un unico modo per costruire ogni lista).

    \begin{gframe}[colframe=Mahogany, colbacktitle=Mahogany]{liste infinite}
        Le liste infinite non possono essere un'algebra induttiva, in quanto contengono una sotto-algebra induttiva (quella delle liste finite).
    \end{gframe}
\end{gframe}

\begin{gframe}[colframe=Orchid, colbacktitle=Orchid]{i booleani come algebra non induttiva}
    \label{Bool}
    Consideriamo l'algebra \( (B, \text{not}) \), dove \( B = \{0, 1\} \) e not: \(B \to B : b\to \neg b\).

    Notiamo che not è sicuramente iniettiva, e che, poiché è l'unico costruttore, anche la seconda caratteristica delle algebre induttive è rispettata.

    Notiamo però che l'algebra non rispetta il terzo requisito. Se consideriamo infatti \( \emptyset \subseteq B\), notiamo che not è chiusa rispetto ad esso. 

    Infatti, l'implicazione \( x \in \emptyset \implies \text{not}(x) \in \emptyset \) risulta vera per falsificazione della premessa (non esistono elementi in \( \emptyset \)).

    \( (\emptyset, \text{not}) \) è quindi una sotto-algebra induttiva di \( B \), che però è diversa da essa. L'implicazione della terza condizione \( (x \in \emptyset \implies \text{not}(x) \in \emptyset) \implies \emptyset = B \) è falsa, e \( (B, \text{not}) \) non è quindi un'algebra induttiva.
\end{gframe}

\section{Omomorfismi, lemma di Lambek}
\begin{gframe}{digressione - teoria delle categorie}

    Facciamo una piccola parentesi che introduce alcune nozioni di teoria delle categorie (perché è molto interessante).

    La teoria delle categorie studia in modo astratto le strutture matematiche. Una categoria  \ \( \mathcal{C} \) \ consiste di:
    \begin{itemize}
        \item  una classe ob(\( \mathcal{C} \)), i cui elementi sono chiamati \textbf{oggetti}
        \item  una classe mor(\( \mathcal{C} \)), i cui elementi sono chiamati \textbf{morfismi} (o mappe o frecce); ogni morfismo \( f: a \to b \) \ ha associati un unico oggetto sorgente \( a \) e un unico oggetto destinazione \( b \).
        \item per ogni terna di oggetti \( a, b, c \in \mathcal{C}\), è definita una funzione \( \text{mor}(b, c) \times \text{mor}(a,b) \to \text{mor}(a, c) \) chiamata \textbf{composizione di morfismi}. La composizione di \( f: b \to c \) con \( g: a \to b \) si indica con \( f \circ g: a \to c \)
            \subitem la composizione deve soddisfare i seguenti assiomi:

            (\textit{associatività}): se \( f: a \to b, \ g: b \to a \text{ e } h: c \to d\), allora \( h \circ (g \circ f) = (h \circ g) \circ f \)

            (\textit{identità}): per ogni oggetto \( x \) esiste un morfismo \( \text{id}_x : x \to x \) chiamato \textbf{morfismo identità}, tale che per ogni morfismo \( f: a \to x \) vale \( \text{id}_x \circ f = f \) e per ogni morfismo \( g : x \to b \) si ha \( g \circ \text{id}_x = g \).
    \end{itemize}

    Quindi, ogni oggetto è associato ad un unico morfismo identità. Questo permette di dare una definizione di categoria basata esclusivamente sulla classe dei morfismi: gli \textbf{oggetti vengono identificati con i corrispondenti morfismi identità}.

    All'interno della teoria delle categorie, una funzione iniettiva \( f: B \to C \) si chiama \textbf{monomorfismo}. Visto che non si possono utilizzare gli elementi per definire l'iniettività, un monomorfismo è descritto come una funzione \( f \) tale che:
    \[
        \forall A, \ \forall h, k : A \to B, \ \ \ h \circ f = k \circ f \implies h = k
    \]
    (se le funzioni \( h \) e \( k \) sono identiche ogni volta che vengono composte con \( f \), significa che non ci sono valori in \( f \) che sono assunti da più di un elemento di \( B \))
\end{gframe}

\begin{defframe}{Algebre con la stessa segnatura}{}
    Due algebre \( (A, \Gamma_A) \) e \( (B, \Gamma_B) \) hanno la stessa segnatura se, sostituendo \( A \) con \( B \) in tutte le \( \gamma_i \in \Gamma_A \), si ottiene \( \Gamma_B \).

    (La segnatura di un'algebra è data dalle segnature delle sue operazioni).
\end{defframe}

\begin{defframe}{Omomorfismo}{}
    Date due algebre con la stessa segnatura \( (A, \Gamma) \) e \( (B, \Delta = \{\delta_1, \dots, \delta_k\} ) \), un omomorfismo è una funzione \( f: A \to B \) tale che:
    \[ \forall i \ \ f(\gamma_i (a_1, \dots, a_k, k_1, \dots, k_m)) = \delta_i (f(a_1), \dots, f(a_k), k_1, \dots, k_m)\]
    {\small(con \( k_1, \dots, k_m \) parametri esterni)}

    \vspace{0.5em}

    (definizione algebrica: \( \forall a, b \in A \), date \(\circ\) operazione di \( A \) e \(\bullet\) operazione di \( B \), si ha \( f(a \circ b) = f(a) \bullet f(b) \) )
    \vspace{0.5em}

    {\small \color{gray} un omomorfismo ``rispetta le operazioni''}

    \begin{itemize}
        \item nota: la composizione di due omomorfismi è a sua volta un omomorfismo
    \end{itemize}
\end{defframe}

\begin{defframe}{Isomorfismo}{}
    Un isomorfismo è un omomorfismo biettivo.

    (Due algebre sono isomorfe (\( \cong \)) quando esiste un isomorfismo tra loro)
\end{defframe}

\begin{thmframe}{Omomorfismo tra algebre con stessa segnatura}{}
    \label{teo-om}
    Sia \( A \) un'algebra induttiva. Per ogni algebra \( B \) (non necessariamente induttiva) con la stessa segnatura, esiste un \textbf{unico omoformismo} \( A \to B \).

\end{thmframe}

\begin{thmframe}{Lemma di Lambek}{}
    Due algebre induttive \( A \) e \( B \) con la \textbf{stessa segnatura} sono necessariamente \textbf{isomorfe}.
\end{thmframe}
    \begin{proofframe}{}
        \begin{itemize}
            \item Siccome \( A \) è un'algebra induttiva, \( \exists! \text{ omomorfismo } f : A \to B \). 
            \item Allo stesso modo, \( \exists! \text{ omomorfismo } g : B \to A \).
            \item Componendo i due omomorfismi, si ottiene un omomorfismo \( g \circ f \) con segnatura \( A \to A \). 

            \item Sappiamo che per ogni algebra esiste l'omomorfismo ``identità''.
            \item Sappiamo anche, per il teorema sopra, che esiste un unico omomorfismo \( A \to A \). 
                \subitem Ne segue necessariamente che \( g \circ a = \text{Id}_A \). {\small (lo stesso discorso si applica a \( f \circ g = \text{Id}_B \))}

            \item  \( g \circ f = \text{Id} \iff g = f^{-1}\), quindi \( g \) e \( f \) sono funzioni invertibili (= biettive) \( \implies \) \( g, f \) sono isomorfismi \( \implies A \cong B\)  

        \end{itemize}

    \end{proofframe}
l


\chapter{Paradigma funzionale}

\section{Linguaggi}
Definiamo un \textbf{linguaggio} \( L \) come insieme di stringhe. 

Per descrivere la sintassi di linguaggi formali (la grammatica), usiamo la BNF (Backus-Naur Form), con questa sintassi:
\[
    \texttt{<simbolo> ::= \_\_espressione\_\_}
\]

\textbf{Esempio}:
prendiamo come esempio questa grammatica:
\[
    M, N ::= 5 \mid 7 \mid M + N \mid M * N
\]

Le espressioni che seguono questa grammatica, sono del tipo:
\begin{itemize}
    \item ``5'' o ``7'' 
    \item un'espressione \( M+N \) o \( M*N \), in cui \( M \) e \( N \) rispettano a loro volta la grammatica
\end{itemize}

Introduciamo una funzione \( eval : L \to \mathbb{N}\), che valuta le espressioni del linguaggio:
\begin{itemize}
    \item \(eval(5) = 5\)
    \item \( eval(7) = 7 \)
    \item \( eval(M+N) = eval(M)+eval(N)\)
    \item \( eval(M*N) = eval(M)*eval(N)\)
\end{itemize}

Possiamo notare subito che \( (L, eval) \) non è un'algebra induttiva. Infatti, una stringa come ``\( 5 + 7 * 5 \)'' potrebbe essere stata generata in due modi diversi: \( (5 + 7) * 5 \) e \( 5 + (7 * 5) \). 

Possiamo però stipulare che sia induttiva. Ci basta infatti considerare \( +, \ *, \ 5\) e \( 7 \) come costruttori dell'algebra. In questo modo, \( (5 + 7) * 5 \) risulta essere un oggetto diverso da \( 5 + (7 * 5) \). È quindi possibile dimostrare che \( (L, 5, 7, +, *) \) è un'algebra induttiva.

\section{Exp}

\begin{defframe}{Linguaggio Exp}{}

    Introduciamo il linguaggio \( Exp \), con grammatica:
    \[
        M, N ::= k \mid x \mid M + N \mid let \ x = M \ in \ N
    \]
    dove:

    \begin{itemize}
        \item \( k \in Val=\{0, 1, \dots\}\) \ è una costante
        \item \( x \in Var \) è una variabile     
        \item \( M + N : Exp \times Exp \to Exp \) \ è la somma tra due espressioni
        \item \(  let : Var \times Exp \times Exp \to Exp\) \ assegna alla variabile \( x \)
            il valore \( M \) all'interno di \( N \)
    \end{itemize}

\end{defframe}

esempi:
\begin{itemize}
    \item \( let \ x = 3 \ in \ x+x+2 \) viene valutata come \( 8 \)
    \item \( let \ x = 3 \ in \ 12 \) viene valutata come \( 12 \)
\end{itemize}

\vspace{0.5em}

Questo linguaggio causa però facilmente ambiguità. Per esempio, come valutiamo un'espressione come \( let \ x = 3 \ in \ let \ y = x \ in \ let \ x = 5 \ in \ y\) ? 

Per esplicitare la struttura del termine, è necessario legare le occorrenze delle variabili alle dichiarazioni.

\begin{defframe}{Variabili libere, legate, scope}{}
    Si dice che un'occorrenza di una variabile \( x \) è \textbf{libera} in un termine \( t \) quando non compare nel corpo di \( N \) nessun sottotermine di \( t \) nella forma \( let \ x = M \ in \ N \) (quindi, quando non le viene assegnato un valore).

    Ogni occorrenza libera di \ \( x \) \ in un termine \( N \) si dice \textbf{legata} (bound) alla dichiarazione di \( x \) nel termine \( let \ x = M \ in \ N \).

    Lo scope di una dichiarazione è l'insieme delle occorrenze libere di \( x \) in \( N \). 

    Lo \textbf{scope di una variabile} è la porzione di programma all'interno della quale una variabile può essere riferita.

\end{defframe}

Introduciamo una funzione \( free: Exp \to \mathcal{P}(Var) \), che restituisce l'insieme delle variabili libere di un'espressione:
\begin{align*}
    free(k) &= \emptyset \\
    free(x) &= \{x\} \\
    free(M+N) &= free(M) \cup free(N) \\
    free(let \ x = M \ in \ N) &= free(M) \cup (free(N) - \{x\}) 
\end{align*}

{\color{gray}(eliminiamo la \( x \), dalle variabili libere in \( N \) perché viene dichiarata dal \( let \ x \), ma non la eliminiamo da \( M \) perché  potrebbe comparire al suo interno come variabile libera, e \( M \) non fa parte dello scope di \( let \ x \) (esempio: in \( let \ x = x \ in \ x \), la \( x \) è libera perché compare libera in \( = x \) ))}

esempio: \( free(let \ x = 7 \ in \ x+y) = \{y\}\)

\subsection{Semantica operazionale}

Vogliamo introdurre nel linguaggio \( Exp \) il concetto di ``quanto fa?'' (valutazione di un'espressione).

Per farlo, abbiamo bisogno di definire un ambiente all'interno del quale valutare le espressioni (stile operazionale, ``structural operational semantics'').

\begin{defframe}{Ambienti}{}
    Un \textbf{ambiente} è una funzione parziale (funzione non necessariamente definita su tutti gli elementi del dominio) con dominio finito che associa dei valori ad un insieme finito di variabili.
    \[ E : Var \ \farrow \ Val  \]

    Scriviamo gli ambienti come insiemi di coppie. Per esempio, l'ambiente \( E \) in cui \( z \) vale \( 3 \) e \( y \) vale \( 9 \) è indicato con \( \{(z, 3), (y, 9)\} \).

    Notiamo che, essendo \( E \) una funzione parziale, il dominio \( dom(E) \) è un sottoinsieme finito di \( Var \).

    \begin{defframe}[colframe=DarkOrchid, colbacktitle=DarkOrchid]{Insieme di ambienti}{}
        \( Env \) è definito come l'insieme degli ambienti di \( Exp \).

    \end{defframe}

\end{defframe}

\vspace{0.5em}

Gli ambienti si possono \textbf{concatenare} in questo modo:
\[ (E_1 E_2)(x) = \begin{cases}
    E_2(x) & \text{se } x\in dom(E_2) \\
    E_1(x) & \text{altrimenti}
\end{cases}
\]

Per esempio, \( \{(z, 3), (y, 9)\}\{(z, 4)\}(z) = 4 \) e \( \{(z, 3), (y, 9)\}\{(z, 4)\}(x) \) è indefinito.

\vspace{0.5em}

\begin{defframe}{Semantica operazionale di \( Exp \)}{}
    La \textbf{semantica operazionale} di \( Exp \) è una relazione
    \[ \leadsto \ \subseteq Exp \times Env \times Val\]

    in cui \( (M, E, v) \in \leadsto \iff \) il programma \( M \), nell'ambiente \( E \), produce il valore \( v \).

    \vspace{0.5em}

    Un'asserzione di appartenenza \( (M, E, v) \in \leadsto\) viene chiamata \textit{giudizio operazionale}, e si scrive
    \[ E \vdash M \leadsto v \]


    Questa relazione è definita dalle seguenti \textbf{regole}:
    \[ E \vdash k \leadsto k  \ \ \ [ const ] \] 
    \vspace{-1.5em}
    {\small\color{gray}\[ (\text{in ogni ambiente } E \text{, una costante } k \text{ vale } k )\]}
    \vspace{-1em}
    \[ E \vdash x \leadsto v  \ \ \ \text{ se }  v = E(x) \ \ \ [ var ] \]
    \vspace{-1.5em}
    {\small\color{gray}\[ (\text{una variabile } x \text{ vale } v \text{ se la funzione ambiente } E(x) \text{ le associa il valore } v )\]}
    \vspace{-0.5em}
    \[
        \frac{
            E \vdash M \leadsto v \quad E \vdash N \leadsto w
        }{
            E \vdash M + N \leadsto v + w
        }
        \ \ \ [plus]
    \]
    {\small\color{gray}\[ (\text{se nello stesso ambiente } M \text{ vale } v \text{ e } N \text{ vale } w, \ M+N \text{ varrà } u+w)\]}
    \vspace{-0.5em}
    \[
        \frac{
            E \vdash M \leadsto v_1 \quad E\{(x, v_1)\} \vdash N \leadsto v_2
        }{
            E \vdash \ let \ x = M \ in \ N \leadsto v_2
        }
        \ \ \  [let]
    \]

    {\color{gray}(essenzialmente, per valutare una \( let \), si:
        \begin{itemize}
            \item valuta \( M\) \ \ (\( E \vdash M \leadsto v_1) \)
            \item si ``associa'' il risultato \( v_1 \) a \( x \), concatenando \( (x, v_1) \) all'ambiente
            \item e si valuta \( N \) nel nuovo ambiente)
    \end{itemize} }
\end{defframe}

Notiamo che si utilizza la relazione \( \leadsto \) e non una una funzione \( Exp \times Env \to Val \), perché si potrebbe avere più di un risultato (per esempio nel caso del multithreading, in cui un diverso ordine di esecuzione di un programma dà output diversi), o anche nessun risultato (per esempio nel caso in cui in \( Exp \) compare una variabile \( x \), che però \( Env \) non definisce), entrambi casi non accettati dalla definizione di funzione. \\[6pt]


\begin{gframe}{precedenza}
    Introduciamo un concetto di ``precedenza'' nella valutazione di un'espressione potenzialmente ambigua;
    un'espressione del tipo:
    \[ let \ \ x = 3 \ \ in \ \ let \ \ x = \ let \ \ y = 2 \ \ in \ \ x+y \ \ in \ \ x+7 + x \]

    in assenza di parentesi, va valutata ``partendo dall'interno''.

    Corrisponde quindi a 

    \[ let \ \ {\color{purple}x}= {\color{purple} 3} \ \ in \ \ [ \ let \ \ {\color{ForestGreen}x}= \ (let \ \ {\color{RoyalPurple}y} = {\color{RoyalPurple} 2} \ \ in \ \ {\color{purple} x}+{\color{RoyalPurple} y}) \ \ in \ \ {\color{ForestGreen}x}+7 \ ] + {\color{purple}x} \]
    E si ha quindi che:
    \begin{itemize}
        \item la \( \color{purple} x \) in \( {\color{purple}x}+y \) e quella finale (\( + \color{purple} x \)) sono quelle valutate dal \( let \) iniziale
        \item il valore della \( \color{ForestGreen} x \) in \( x+7 \) è invece dato dal risultato di \(let \ x =  (let \  y = {\color{purple} 2} \ \ in \ \ {\color{purple} x}+{\color{purple} y})  \)
    \end{itemize}

\end{gframe}

Facciamo un esempio di valutazione di un'espressione:


\begin{center}
\includegraphics[width=1\textwidth]{img/valutazione-es}
\end{center}
(copierò appena ho tempo...)

\section{Valutazioni Eager e Lazy}
La valutazione utilizzata fino a questo momento viene definita
\textbf{eager}, in quanto valuta \( N \) immediatamente (anche nel caso in cui non servisse veramente valutarlo).

Se infatti consideriamo un caso del tipo
\( let \ = \text{ [espressione lunghissima] } \ in \ 7 \), notiamo immediatamente che la valutazione di \( N \) non è necessaria, in quanto l'espressione farà, in ogni caso, \( 7 \).

Introduciamo quindi un approccio \textbf{lazy}, che consiste nel valutare un termine solo quando (e se) ce n'è veramente bisogno. 

La valutazione di \( N \) in un termine del tipo \( let \ x = \ N \ in \ M \) viene rimandata, quindi, al momento in cui ad \( M \) (eventualmente) servirà il suo valore.

\begin{defframe}{Regole della semantica lazy di \(Exp\)}{}
   \begin{itemize}
    \item I termini non valutati subito vengono conservati in un ``ambiente pigro'' - estendiamo quindi \( Env \) in questo modo:
   \[ Env = Var \farrow Exp \]
   (gli ambienti contengono ora anche i termini non valutati, quindi non possiamo avere come codominio \( Val \))

\item la nuova regola per le variabili è:
    \[ \frac{E\vdash M \leadsto v}{E\vdash x \leadsto v} \ \ \text{se } E(x)=M\]

\item la nuova regola per il \( let \) è:
    \[ \frac{E(x, M)\vdash N \leadsto v}{E \vdash let \ x = M \ in \ N \leadsto v} \]

   \end{itemize}
\end{defframe}

Notiamo che però non sempre l'approccio lazy è più veloce: per esempio, per l'espressione \( let \ x = N \ in \ (x+x+x) \), \( N \) viene calcolata \( 3 \) volte con l'approccio lazy e una sola con quello eager.


Mettiamo i due approcci a confronto sull'espressione \[ let \ x = 2 \ in \ let \ y = x \ in \ let \ x = 7 \ in \ y \leadsto 3 \]
  
  \begin{itemize}
      \item \textbf{approccio eager}:
  \end{itemize}
  \vspace{-1em}
\begin{prooftree}
\AxiomC{$(x,2) \vdash x \rightsquigarrow 2$}
\AxiomC{$(x,2) \vdash 1 \rightsquigarrow 1$}
\BinaryInfC{$(x,2) \vdash x + 1 \rightsquigarrow 3$}
\AxiomC{$(x,2)(y,3) \vdash 7 \rightsquigarrow 7$}
\AxiomC{$(x,2)(y,3)(x,7) \vdash y \rightsquigarrow 3$}
\BinaryInfC{$(x,2)(y,3) \vdash \text{let } x = 7 \text{ in } y \rightsquigarrow 3$}
\BinaryInfC{$(x,2) \vdash \text{let } y = x + 1 \text{ in let } x = 7 \text{ in } y \rightsquigarrow 3$}
\UnaryInfC{$\emptyset \vdash \text{let } x = 2 \text{ in let } y = x + 1 \text{ in let } x = 7 \text{ in } y \rightsquigarrow 3$}
\end{prooftree}

  \begin{itemize}
      \item \textbf{approccio lazy}:
  \end{itemize}
  \vspace{-1em}

\begin{prooftree}
\AxiomC{$(x,2)(y,x+1)(x,7) \vdash 7 \rightsquigarrow 7$}
\UnaryInfC{$(x,2)(y,x+1)(x,7) \vdash x \rightsquigarrow 7$}
\AxiomC{$(x,2)(y,x+1)(x,7) \vdash 1 \rightsquigarrow 1$}
\BinaryInfC{$(x,2)(y,x+1)(x,7) \vdash x + 1 \rightsquigarrow 8$}
\UnaryInfC{$(x,2)(y,x+1)(x,7) \vdash y \rightsquigarrow 8$}
\UnaryInfC{$(x,2)(y,x+1) \ \text{let } x = 7 \text{ in } y \rightsquigarrow 8$}
\UnaryInfC{$(x,2) \vdash \text{let } y = x + 1 \text{ in let } x = 7 \text{ in } y \rightsquigarrow 8$}
\UnaryInfC{$\emptyset \vdash \text{let } x = 2 \text{ in let } y = x + 1 \text{ in let } x = 7 \text{ in } y \rightsquigarrow 8$}
\end{prooftree}

Notiamo che i due approcci ci danno risultati diversi.

Ciò è causato non dall'approccio valutativo, bensì dallo \textbf{scoping} utilizzato. Abbiamo infatti utilizzato quello che viene definito "scoping dinamico", il che ha causato problemi perché, in \( Exp \), lazy dinamico e eager non sono equivalenti.

\pagebreak

\section{Scoping}

\begin{defframe}{Scoping}{}
    Lo \textbf{scoping} di un linguaggio è l'insieme di regole che determinano la visibilità di una variabile all'interno di un programma (ossia che consentono di associare una variabile a ciascun riferimento {\small \color{gray} (= uso della variabile mediante un identificatore)}).
\end{defframe}

\begin{defframe}{Scoping statico}{}
    Quando si usa lo \textbf{scoping statico}, i riferimenti ad una variabile sono risolti in base alla \textbf{struttura sintattica} del programma (tipicamente in base ad una dichiarazione).

    Ovvero, durante la valutazione viene utilizzato l'\textbf{ambiente definito a tempo di interpretazione} (e non di valutazione).
\end{defframe}

\begin{defframe}{Scoping dinamico}{}
    Quando si usa lo \textbf{scoping dinamico}, i riferimenti ad una variabile sono risolti in base allo \textbf{stato di esecuzione} del programma (per esempio, una dichiarazione estende il suo effetto fino a che non si incontra un altra dichiarazione di variabile con lo stesso nome).

    Quindi, durante la valutazione viene utilizzato l'\textbf{ambiente definito a tempo di valutazione} stesso.
\end{defframe}

Dobbiamo quindi mantenere, oltre alle espressioni rimaste da valutare, anche gli ambienti in cui valutarle.

Per farlo, estendiamo nuovamente \( Env \) in questo modo:
\[ Env_{LS} = Var \farrow (Exp \times Env_{LS}) \]


\begin{defframe}{Regole della semantica lazy statica di \(Exp\)}{}

   \begin{itemize}
    \item I termini non valutati subito vengono conservati in un ``ambiente pigro'' - estendiamo quindi \( Env \) in questo modo:
   \[ Env = Var \farrow Exp \]
   (gli ambienti contengono ora anche i termini non valutati, quindi non possiamo avere come codominio \( Val \))

\item la nuova regola per le variabili è:
    \[ \frac{E'\vdash M \leadsto v}{E\vdash x \leadsto v} \ \ \text{se } E(x)=(M, E')\]

\item la nuova regola per il \( let \) è:
    \[ \frac{E(x, M, E)\vdash N \leadsto v}{E \vdash let \ x = M \ in \ N \leadsto v} \]

   \end{itemize}
\end{defframe}

Valutiamo la stessa espressione anche con questo approccio:

\begin{prooftree}
\AxiomC{$(x,2,\emptyset) \vdash x\leadsto 2$}
\AxiomC{$(x,2,\emptyset) \vdash 1 \leadsto 1 $}
\BinaryInfC{$(x,2, \emptyset)\vdash x + 1 \rightsquigarrow 3$}
\UnaryInfC{${\color{blue}E}(x, 7,E) \vdash y \rightsquigarrow 3$}
\UnaryInfC{${\color{blue}(x,2, \emptyset)(y,x+1, (x, 2, \emptyset)}\vdash \ \text{let } x = 7 \text{ in } y \rightsquigarrow 3$}
\UnaryInfC{$(x,2,\emptyset) \vdash \text{let } y = x + 1 \text{ in let } x = 7 \text{ in } y \rightsquigarrow 3$}
\UnaryInfC{$\emptyset \vdash \text{let } x = 2 \text{ in let } y = x + 1 \text{ in let } x = 7 \text{ in } y \rightsquigarrow 3$}
\end{prooftree}

In \( Exp \) non c'è, invece, differenza tra eager statico e eager dinamico.

\begin{gframe}{}
   Essenzialmente, in \( Exp \):
   \vspace{-1em}
\begin{center}
\begin{tabular}{c|c|c}
     & \textbf{statico} & \textbf{dinamico} \\ \hline
\textbf{lazy} & equiv & non equiv \\ \hline
\textbf{eager}& \multicolumn{2}{c}{equiv (e uguali tra loro)} \\
\end{tabular}
\end{center} 
\end{gframe}

\begin{gframe}[colframe=DeepGrey, colback=DeepGreyLight, colbacktitle=DeepGrey]{``commutatività'' in \( Exp \)}
    In \( Exp \), si ha:
    \[ let \ x = (let \ y  = M \ in \ N) \ in \ L \not\equiv \ let \ y = M \ in \ let \ x = N \ in \ L \]
\begin{itemize}
    \item nella prima espressione, \( y \) è definita solo all'interno di \( N \)
    \item nella seconda, è definita prima, ed è quindi visibile anche in \( L \)
    \item quindi, le due espressioni sono equivalenti solo se \( y \) non compare libera {\small \color{gray}(non ri-definita)} in \( L \)
\end{itemize}
\end{gframe}

\subsection{Riassunto delle regole in \( Exp \)}

\begin{gframe}[colframe=DeepGreen, colback=DeepGreenLight, colbacktitle=DeepGreen]{eager}
        \[ E \vdash k \leadsto k  \ \ \ [ const ] \] 
        \vspace{-2.5em}

    \[ E \vdash x \leadsto v  \ \ \ \text{ se }  v = E(x) \ \ \ [ var ] \]
        \vspace{-2em}

    \[
        \frac{
            E \vdash M \leadsto v \quad E \vdash N \leadsto w
        }{
            E \vdash M + N \leadsto v + w
        }
        \ \ \ [plus]
    \]



    \[
        \frac{
            E \vdash M \leadsto v_1 \quad E\{(x, v_1)\} \vdash N \leadsto v_2
        }{
            E \vdash \ let \ x = M \ in \ N \leadsto v_2
        }
        \ \ \  [let]
    \]


\end{gframe}


\begin{gframe}[colback=DeepTealLight]{lazy statico}
    
    \[ \frac{E'\vdash M \leadsto v}{E\vdash x \leadsto v} \ \ \text{se } E(x)=(M, E') \ \ \ [var] \]

    \[ \frac{E(x, M, E)\vdash N \leadsto v}{E \vdash let \ x = M \ in \ N \leadsto v} \ \ \ [let]\]


\end{gframe}


\begin{gframe}[colframe=BlueGray, colback=BlueGrayLight, colbacktitle=BlueGray]{(lazy dinamico)}
    \[ \frac{E\vdash M \leadsto v}{E\vdash x \leadsto v} \ \ \text{se } E(x)=M \ \ \ [var]\]

    \[ \frac{E(x, M)\vdash N \leadsto v}{E \vdash let \ x = M \ in \ N \leadsto v} \ \ \ [let] \]
\end{gframe}


\todo{esercizi}

\pagebreak

\section{Fun}
Introduciamo un nuovo linguaggio, \( Fun \), che estende \( Exp \) con la nozione di \textbf{funzione}.

\begin{defframe}{\( Fun \)}{}

La grammatica di \( Fun \) è:
    \[
        M, N ::= k \mid x \mid M + N \mid let \ x = M \ in \ N \mid fn \ x \implies M \mid MN
    \]
    dove:
    \begin{itemize}
        \item le regole presenti in \( Exp \) (1-4) rimangono invariate, con gli appropriati cambi di dominio (es. \( let : Var \times Fun \times Fun \to Fun \))
        \item \( fn : Var \times Fun \to Fun \) \ è una \textbf{funzione} (anonima) con parametro \( x \)     
        \begin{itemize}
            \item una funzione \( fn \ x \implies M \) si può rappresentare in maniera alternativa attraverso la sua \textbf{chiusura}, \( (x, M)\in Var \times Fun \)
        \end{itemize}
    \item \( \cdot : Fun \times Fun \to Fun \) \ è l'\textbf{applicazione di funzioni} 
        \subitem il termine sinistro (che, perché l'espressione abbia semanticamente senso, deve necessariamente essere una funzione) viene applicato al termine destro (quindi \( MN=M(N) \))
    \item l'insieme \( Val \) non coincide più con quello delle costanti, ma corrisponde a \( Var \cup (Var \times Fun) \) (variabili \( \cup \) chiusure)
    \end{itemize}

\end{defframe}

Quindi, per esempio:
\begin{itemize}
    \item \( (fn \ x \implies x+1) \ 5 = 6\) \ \ (la funzione \( x+1 \) è applicata all'argomento \( 6 \))
    \item \( (fn \ x \implies x \ 5)(fn \ y \implies y + 1) = 6 \)
        \subitem la funzione prende in input una funzione (in questo caso ``successore''), e la applica a \( 5 \)
    \item \( (fn \ x \implies (fn \ y \implies y \ x)) \ 3 \ (fn \ z \implies z+1) \ = 4\)
        \subitem è una funzione che, presa in input un'altra funzione, la applica ad \( x \) - le passiamo la funzione ``successore'', che, applicata a \( 3 \), dà \( 4 \).
    \item un'applicazione del tipo \( fn \ x \implies x \ 10 \) ``non ha semantica'' (non è valutabile), in quanto \( 10 \) non è una funzione e non si può applicare a \( x \)
\end{itemize}

\begin{gframe}{precedenza di \( apply \)}
    la precedenza nell'applicazione è a sinistra
    \[ MNL \equiv (MN)L \]
\end{gframe}

\begin{cdefframe}{DeepGreenLight}{DeepGreen} {Semantica eager dinamica di \( Fun \)}{}
    
\begin{center}
    [\( fn \) dinamico eager]
\end{center}
\vspace{-1em}
    \[  E \vdash fn \ x \implies M \leadsto (x, M) \]

\begin{center}
    [\( apply \) dinamico eager]
\end{center}
\vspace{-2.3em}
\begin{prooftree}
\AxiomC{$E \vdash M \leadsto (x, M')$}
\AxiomC{$E\vdash N \leadsto v$}
\AxiomC{$E (x, v)\vdash M' \leadsto v'$} 
\TrinaryInfC{$E \vdash MN \leadsto v'$ }
\end{prooftree}
    
\end{cdefframe}

Esempio:
\todo{esempio}

\begin{cdefframe}{DeepGreenLight}{DeepGreen}{Semantica eager statica di \( Fun \)}{}
    \begin{center}
    [\( fn \) statico eager]
\end{center}
\vspace{-1em}
    \[  E \vdash fn \ x \implies M \leadsto (x, M, E)\]

\begin{center}
    [\( apply \) statico eager]
\end{center}
\vspace{-2.3em}
\begin{prooftree}
\AxiomC{$E \vdash M \leadsto (x, M', E')$}
\AxiomC{$E\vdash N \leadsto v$}
\AxiomC{$E' (x, v)\vdash M' \leadsto v'$} 
\TrinaryInfC{$E \vdash MN \leadsto v'$ }
\end{prooftree}

\end{cdefframe}


\begin{lemmaframe}{Eager dinamico e statico in \( Fun \)}{}
    (Al contrario di \( Exp \)), si ha che:
    \[ Fun \text{ eager dinamico} \not\equiv \ Fun \text{ eager statico } \]
\end{lemmaframe}



\begin{defframe}{Semantica lazy dinamica di \( Fun \)}{}

    \begin{center}
        [\( fn \) lazy statico]
    \end{center}
    \vspace{-1em}
    \[ E \vdash fn \ x \implies M \leadsto (x, M) \]


    \begin{center}
        [\( apply \) lazy statico]
    \end{center}
    \vspace{-2em}
    \begin{prooftree}
        \AxiomC{\( E\vdash M \leadsto (x, M') \)}
        \AxiomC{\( E(x, N)\vdash M' \leadsto v \)}
        \BinaryInfC{\( E\vdash M \ N \leadsto v \)}
    \end{prooftree}
\end{defframe}
\begin{defframe}{Semantica lazy statica di \( Fun \)}{}

    \begin{center}
        [\( fn \) lazy statico]
    \end{center}
    \vspace{-1em}
    \[ E \vdash fn \ x \implies M \leadsto (x, (M, E)) \]


    \begin{center}
        [\( apply \) lazy statico]
    \end{center}
    \vspace{-2em}
    \begin{prooftree}
        \AxiomC{\( E\vdash M \leadsto (x, M', E') \)}
        \AxiomC{\( E'(x, (N, E))\vdash M' \leadsto v \)}
        \BinaryInfC{\( E\vdash M \ N \leadsto v \)}
    \end{prooftree}
\end{defframe}

Introduciamo un termine interessante - \( (fn \ x \implies xx)(fn \ x \implies xx) \) - e tentiamo di valutarlo con un approccio eager (dinamico)
\begin{prooftree}
    \def\defaultHypSeparation{\hskip-5pt}
  \AxiomC{\(  \emptyset \vdash fn \ x \implies xx \leadsto (x, xx) \)}
  \AxiomC{\(  (x, (x,xx))\vdash  x \leadsto (x, xx) \ \)}
  \AxiomC{\((x, (x,xx))\vdash  x \leadsto (x, xx) \quad (x, (x,xx))\vdash  xx \leadsto \)}
  \BinaryInfC{\( \emptyset \vdash fn \ x \implies xx \leadsto (x, xx) \quad (x, (x,xx))\vdash  xx \leadsto \)}
  \BinaryInfC{\( \emptyset \vdash (fn \ x \implies xx)(fn \ x \implies xx) \)}
\end{prooftree}


Notiamo che il termine va in loop - infatti, si ha che, per valutare \((x, (x,xx))\vdash  xx \), bisogna prima valutare \((x, (x,xx))\vdash  xx \) (se stesso).

\begin{gframe}{Termine \( \omega \)}
    Chiamiamo \( \omega \) il termine appena introdotto:
    \[ \omega =  (fn \ x \implies xx)(fn \ x \implies xx) \]

\end{gframe}

Qui emerge una grande differenza tra valutazione eager e lazy: con un approccio eager, un'espressione del tipo \( let \ x = \omega \ in \ 7 \) va in loop, mentre con una valutazione lazy viene valutata correttamente.


\begin{defframe}{Curryficazione}{}

    La \textbf{curryficazione} (o ``applicazione parziale'') è la tecnica che consiste nel tradurre una funzione che accetta più argomenti in una sequenza di famiglie di funzioni, ciascuna delle quali accetta un singolo argomento.


    Partendo da una funzione \( f: (X \times Y) \to Z\) che prende due argomenti, la sua curryficazione tratta il primo argomento come un parametro, e crea una famiglia di funzioni \( f_x : Y \to Z \) tale che, per ogni \( x \in X  \), c'è esattamente una funzione \( f_x \) tale che \( \forall \ y \in Y , \  f_x(y)=f(x,y)\).
    \[ \text{curry}: [ \ (X \times Y) \to Z \ ] \to [ \ X \to (Y \to Z) \ ]\]
    \[ f \mapsto h : f(x, y) = h(x)(y) \]


Si trasforma quindi una funzione che prende due argomenti in una funzione ad un argomento che ritorna un'altra funzione t.c. \( curry((f))(x)(y) = f(x, y) \).

(Il processo inverso prende il nome di \textbf{decurryficazione}).

\end{defframe}

\begin{cframe}{DeepGreenLight}{DeepGreen}{Curryficazione in \( Fun \)}
    La curryficazione ci permette di introdurre una notazione contratta del \( fn \):
    \[ [ \ fn \ x y \implies \ ] \equiv [ \ fn \ x \implies ( fn \ y \implies ) \ ]\]

    Possiamo così introdurre \textbf{funzioni a più argomenti} all'interno del linguaggio \( Fun \).

\end{cframe}

\begin{lemmaframe}{Eager dinamico e statico in \( Fun \)}{}
    (Al contrario di \( Exp \)), si ha che:
    \[ Fun \text{ eager dinamico} \not\equiv \ Fun \text{ eager statico } \]
\end{lemmaframe}

\todo{ESEMPIO}
\begin{prooftree}
  \AxiomC{$Y$}
  \AxiomC{$X$}
  \AxiomC{$A$}
  \AxiomC{$B$}
  \AxiomC{$C$}
  \TrinaryInfC{$Z$}
  \TrinaryInfC{$F$}
\end{prooftree}

\begin{defframe}{Semantica operazionale di \( Fun \) lazy dinamico}{}

    \begin{center}
        [\( fn \) lazy statico]
    \end{center}
    \vspace{-1em}
    \[ E \vdash fn \ x \implies M \leadsto (x, M) \]


    \begin{center}
        [\( apply \) lazy statico]
    \end{center}
    \vspace{-2em}
    \begin{prooftree}
        \AxiomC{\( E\vdash M \leadsto (x, M') \)}
        \AxiomC{\( E(x, N)\vdash M' \leadsto v \)}
        \BinaryInfC{\( E\vdash M \ N \leadsto v \)}
    \end{prooftree}
\end{defframe}
\begin{defframe}{Semantica operazionale di \( Fun \) lazy statico}{}

    \begin{center}
        [\( fn \) lazy statico]
    \end{center}
    \vspace{-1em}
    \[ E \vdash fn \ x \implies M \leadsto (x, (M, E)) \]


    \begin{center}
        [\( apply \) lazy statico]
    \end{center}
    \vspace{-2em}
    \begin{prooftree}
        \AxiomC{\( E\vdash M \leadsto (x, M', E') \)}
        \AxiomC{\( E'(x, (N, E))\vdash M' \leadsto v \)}
        \BinaryInfC{\( E\vdash M \ N \leadsto v \)}
    \end{prooftree}
\end{defframe}

\begin{defframe}{Curryficazione}{}

    La \textbf{curryficazione} è la tecnica che consiste nel tradurre una funzione che accetta più argomenti in una sequenza di famiglie di funzioni, ciascuna delle quali accetta un singolo argomento.


    Partendo da una funzione \( f: (X \times Y) \to Z\) che prende due argomenti, la sua curryficazione tratta il primo argomento come un parametro, e crea una famiglia di funzioni \( f_x : Y \to Z \) tale che, per ogni \( x \in X  \), c'è esattamente una funzione \( f_x \) tale che \( \forall \ y \in Y , \  f_x(y)=f(x,y)\).

    \[ \text{curry}: [ \ (X \times Y) \to Z \ ] \to [ \ X \to (Y \to Z) \ ]\]
    \[ f \mapsto h : f(x, y) = h(x)(y) \]


Si trasforma quindi una funzione che prende due argomenti in una funzione ad un argomento che ritorna un'altra funzione t.c. \( curry((f))(x)(y) = f(x, y) \)

\end{defframe}


\chapter{Lambda calcolo}
\section{Numeri di Church}
Tra i diversi modi di rappresentare i numeri naturali, ci interessa presentare quello di Alonzo Church.

Per Church, il cui mondo è fatto di funzioni, un numero naturale \( n \) corrisponde all'applicare \( n \) volte una funzione \( x \) su un argomento \( y \).
 
Possiamo, per esempio, rappresentare il numero \( 2 \) ``di Church'' in \( Fun \) in questo modo:
\[ fn \  x \ y \implies x \ (x \ y) \ {\color{gray} \equiv fn \ x \implies fn \ y \implies x \ (x \ y) }\]
    (ovvero, presa una funzione \( x \) e un valore \( y \) di partenza, si applica due volte la funzione \( x \) (prima al valore stesso ``(\( x \ y \))'', e poi al risultato di questa applicazione - ``\( x( \)'' \ )))

\begin{defframe}{Numeri di Church in \( Fun \)}{}
    Più in generale, indicando con \( M^n N \) il termine \( M(M(\dots(MN)\dots)) \) {\small\color{gray}(in cui si ripete \( n \) volte \( M \))}, un numero \(c_n\) di Church si può rappresentare, con la sintassi di \( Fun \), in questo modo:
    \[ c_n \equiv fn \ x \ y \implies x^n \ y \]
\end{defframe}

Possiamo rappresentare anche altri concetti essenziali come la funzione ``successore'', la somma e il prodotto tramite numeri di Church.

\begin{defframe}{\( succ \) di Church}{}
    La funzione \textbf{successore di Church}, seguendo lo stesso ragionamento, dovrà ricevere un numero di Church \( z \) in ingresso, e restituire il numero di Church che applica \( x \) a \( y \), \( z+1 \) volte:
    \[ succ \equiv fn \ z \ x \ y \implies x\ (z \ x \ y) \]
    \[\color{gray} succ \equiv fn \ z \implies fn \ x \implies fn \ y \implies x\ (z \ x \ y) \]
    essenzialmente, dato \( z \) numero di Church di cui calcolare il successore (che vuole quindi come parametri \( x \) funzione e \( y \) valore di partenza), si applica una volta in più \( x \).

    La funzione si può scrivere anche, equivalentemente, in questo modo:
    \[ fn \ z \ x \ y \implies z \ x \ (x \ y) \]
    ``anticipando'' essenzialmente il \( +1 \) (prima si applica \( x \) una volta ``in più'', e poi le altre \( z \) volte)
\end{defframe}

Facciamo un esempio concreto:
\begin{align*}
    succ \ c_1 &= fn \ x \ y \implies x \ (c_1 \ x \ y) \\
               & = fn \ x \ y \implies x \ (( fn \ x \ y \implies xy) \ x \ y ) \\
               & = fn \ x \ y \implies x \ ( x \ y)
\end{align*}
che corrisponde al due di Church !

\begin{defframe}{Dechurchificazione}{}
    Perché questo abbia più senso, ci è utile poter ricondurre i numeri di Church all'algebra dei numeri naturali.
    
    Possiamo, con questo scopo, definire una funzione \( dechurch \) (o ``\( eval \,\)''), che, dato un numero di Church \( c_n \), ci restituisce l'intero corrispondente \( n \).

    \[ dechurch(M) = M \ (fn \ x \implies x+1) \ 0 \]

    \begin{gframe}{}
    Quello che stiamo facendo, essenzialmente, è passare al numero di Church in input la funzione ``successore'' dei numeri naturali, e il numero \( 0 \). Il numero di Church applicherà quindi \( n \) volte \( succ() \) a partire da \( 0 \), ritornandoci \( n \).
    \end{gframe}
\end{defframe}

Facciamo un esempio. Dato \( c_2 = fn \ x \ y \implies x \ (x \ y ) \), calcoliamo \( dechurch(c2) \) in questo modo:
\begin{align*}
    dechurch(c2) &= (fn\ x \implies fn \ y \implies x \ (x \ y)) \ (fn \ x \implies x+1) \ 0 \\
    & \text{sostituisco } x: \\
    &= (fn\ y \implies (fn\ x \implies x+1) ((fn\ x \implies x+1) \ y)) \ 0 \\
    & \text{sostituisco } y: \\
    &= (fn\ x \implies x+1) ((fn\ x \implies x+1) \ 0) \\
    & \text{applico:} \\
    &= (fn\ x \implies x+1) (0+1) \\
    &= (fn\ x \implies x+1) \ 1 \\
    &= 1+1 \\
    &= 2
\end{align*}

\begin{thmframe}{}{}
    Si ha:
    \[ dechurch(church(M)) \leadsto k \iff M \leadsto k \]
\end{thmframe}

\begin{defframe}{Somma di Church}{}
    Seguendo lo stesso ragionamento usato per calcolare \( succ \) di Church, la \textbf{somma di Church} tra \( z\) e \( w \) è quella funzione che applica \( x\) \( w\) volte a \( y \), e passa il risultato a \( z \), (che la applicherà altre \( z \) volte).
    \[ plus \equiv fn \ z \ w \ x \ y \implies z \ x \ (w \ x \ y)\]

    (passo a \( z \), numero di Church che vuole quindi una funzione e un valore di partenza, come funzione \( x \) e come valore di partenza l'applicazione di \( x \) a partire da \( y \), \( w \) volte)

\end{defframe}

\begin{defframe}{Prodotto di Church}{}
    Allo stesso modo, possiamo calcolare il \textbf{prodotto di Church}.

    Sappiamo che \( u \times v = \underset{v \text{ volte}}{u + u + u + ... + u} \).

    Sappiamo che un numero di Church ha come parametri una funzione e un valore da cui iniziare. Possiamo quindi ``definire'' una funzione \( plus_u \) che somma \( u \) al suo input: \( fn \ x \implies (plus \ x \ v) \) (in cui \( plus \) è la somma di Church precedentemente definita).

    Ora, ci basta fornire al numero \( v \) come parametri questa nuova funzione e \( c_0 \) come valore di inizio, perché questa, essenzialmente, sommi \( u \) \( v \) volte a partire da zero.
    \[ times \equiv fn \ v \implies fn \ u \implies v \ (fn \ x \implies (plus \ x \ u) ) \ c_0 \]
    \[\color{gray} \equiv fn \ v  \ u \implies v \ (plus_u)  \ c_0 \]
    
\end{defframe}

\section{Primi cenni di SML}
\textbf{Standard ML} (SML) è un linguaggio di programmazione funzionale di alto livello, appartenente alla famiglia di linguaggi \textit{ML} (\textit{Meta Language}). È stato standardizzato negli anni ’90 e si distingue per il suo forte sistema di tipi statico con inferenza automatica.
Il linguaggio discende direttamente da \textit{ML}, sviluppato da Robin Milner nel 1973 come linguaggio di metaprogrammazione per dimostrazioni automatiche.

L'SML di cui tratta questo corso è SML/NJ, lo Standard ML of New Jersey.

\begin{defframe}{Sintassi base di SML/NJ}{}
    Definiamo i costrutti base del linguaggio \( Fun \) in SML.

    \begin{itemize}
        \item definizione di una variabile:
            \begin{lstlisting}
                val x = 10;
                val y = 10 + x;
            \end{lstlisting}

            \item \( let \)
                \begin{lstlisting}
                    val x = let val a = 3 in a + 5 end;
                \end{lstlisting}

            \item \( fn \)
                \begin{lstlisting}
                    val succ = fn x => x+1;
                    fun succ x = x+1; 
                \end{lstlisting}

            \item \( apply \)
                \begin{lstlisting}
                val due = succ 1;
                val tre = succ (succ 1);
                \end{lstlisting}
                \begin{gframe}{}
                    in SML, la precedenza nell'applicazione è a sinistra:
                    \( x \ x \ y  \equiv (x \ x) \ y \)
                \end{gframe}
    \end{itemize}

\end{defframe}

Possiamo ora definire i numeri di Church e le loro operazioni.

\textbf{numeri di Church}:
\begin{lstlisting}
val zero = fn x => fn y => y;
fun zero x y = y;

val uno = fn f => fn x => f x;
fun uno x y = x y;

val due = fn f => fn x => f (f x);
fun due x y = x (x y);
\end{lstlisting}

\textbf{Operazioni}:
\begin{lstlisting}
    val succ = fn w => (fn x => fn y => x(w x y));

    val plus = fn u => fn v => (u succ v);
    val plus u v x y = v x(u x y);

    val times = fn u => fn v => (u (fn z => (plus z v)) zero);
\end{lstlisting}

\section{Ricorsione nel paradigma funzionale: combinatore di punto fisso}
\begin{defframe}{Punto fisso di una funzione}{}
    Un punto fisso per una funzione \( f: A\to A \) (endofunzione) è un elemento \( x \in A \) t.c. \( f(x) = x \).
\end{defframe}

\textbf{Esempio 1}: In un anello \( (A, +, \cdot) \), l'elemento neutro della somma \( 0_A \) è un punto fisso per l'operazione di opposto (l'opposto dell'elemento neutro è se stesso).

\textbf{Esempio 2}: data la funzione \texttt{F} definita in questo modo: 

\texttt{F f \(\equiv \) if (n>0) then 1 else n*f(n-1)}

(funzione che prende in input un'altra funzione \texttt{g} e ritorna la funzione \texttt{if (n>0) then 1 else n*g(n-1)})
{\color{gray}(quindi, se le passiamo in input la funzione ``\texttt{72}'', abbiamo ``\texttt{F 72 \(\equiv \) if (n>0) then 1 else n*72(n-1)}'')}

Qual è il suo punto fisso? Notiamo che è la funzione fattoriale.

Infatti, \texttt{F fact \(\equiv \) if (n>0) then 1 else n*fact(n-1) \( \equiv \) fact} stessa!

\begin{defframe}{Combinatore di punto fisso Y}{}
    (``combinatore paradossale Y'' di Haskell Curry)

    Definiamo come \textbf{combinatore di punto fisso} una funzione che, data una funzione \( F \), trova il suo punto fisso \( YF \).

    Quindi una funzione per cui si ha che \( F(YF) = F \).

    Nel \textit{lambda calcolo},
    \[ Y \equiv fn \ F \implies \bigl[ \left(fn \ x \implies F(xx)\right)\left(fn \ x \implies F(xx)\right) \bigr] \]
    
\end{defframe}

Applichiamo \( Y \) ad una funzione \( F \):
\begin{align*}
    YF \equiv& \bigl((fn \ x \implies F(xx)) (Fn \ x \implies F(xx))\bigr) F  \\
       &\text{sostituiamo il parametro \( F \)} \\
       \brule& \ (fn \ x \implies F(xx) (fn \ x \implies F(xx)) \\
        &(\text{notiamo che abbiamo ottenuto nuovamente } YF) \\
        &\text{sostituiamo il parametro \((fn \ x \implies F(xx))\)} \\
       \brule& \ F\bigl((fn \ x \implies F(xx) (fn \ x \implies F(xx))\bigr) \\
       \equiv&\ F(YF)
  \end{align*}

  Otteniamo quindi che \( YF \equiv F(YF) \).

  Possiamo continuare all'infinito: \( YF = F(YF) = F(F(YF)) = \dots \): abbiamo introdotto la \textbf{ricorsione} nel lambda calcolo.

\begin{thmframe}{Ricorsione nel lambda calcolo}{}
    Grazie al combinatore di punto fisso \( Y \), possiamo ottenere la ricorsione nel lambda calcolo. Infatti, data una funzione \( F \), l'espressione \( YF \) applica \( F \) ricorsivamente.
\end{thmframe}


\chapter{Paradigma imperativo}

\begin{gframe}{}
    Ci è utile conoscere la differenza tra \textbf{call by reference} e \textbf{call by value}:
    \begin{itemize}
        \item call by reference: viene passato alla funzione un riferimento (l'indirizzo) alla variabile originale; qualsiasi modifica interna modifica direttamente il valore originale.
        \item  call by value: viene passata alla funzione una copia del valore della variabile; qualsiasi modifica interna è fatta solo sulla copia e non influisce sul valore originale.
    \end{itemize}


\end{gframe}

\section{\( Imp \): un semplice linguaggio imperativo}
\begin{defframe}{Grammatica del linguaggio \( Imp \)}{}
    \[ 
        k ::= 0 \mid 1 \mid \dots \mid true \mid false
    \]
    \[
        M, N ::= k \mid x \mid M + N \mid M < N
    \]
    \vspace{-2em}
    \begin{align*}
        p, q ::= \ &skip \mid p;q \mid if \ M \ then \ p \ else \ q \mid while \ M \ do \ p \mid \\
                                                                        &var \ x = M \ in \ p \mid x:= M
    \end{align*}
    dove:
    \begin{itemize}
        \item \( M, \ N \in Exp\) 
        \item \( p, \ q \in Imp\) 
        \item \( skip \) è il \textbf{programma che non fa niente}
        \item \( p;q \) indica che verrà eseguito prima \( p \), e poi \( q \)
        \item \( if-then-else \) esegue \( p \) se l'espressione \( M \) è vera e \( q \) altrimenti
        \item \( while \) esegue \( p \) finché \( M \) è vera
        \item \( var \) rappresenta l'\textbf{inizializzazione} di una nuova variabile all'interno di \( p \) (variabile locale in \( p \))
        \item \( := \) rappresenta l'\textbf{assegnamento} di un valore ad una variabile già definita
    \end{itemize}
\end{defframe}

Introduciamo il concetto di \textbf{locazioni} per prepararci al linguaggio \textit{All}. Le locazioni, definite dall'insieme \( Loc \), sono le locazioni di memoria, ovvero gli indirizzi dove si trovano i valori {\color{gray}\small (essenzialmente come i puntatori)}.


\begin{cframe}{DeepBlueLight}{DeepBlue} {Domini semantici}
    Definiamo \( Env \), l'insieme degli ambienti, in questo modo:
    \[ Env = Var \farrow Loc \]
    (un ambiente associa alle variabili, invece dei valori, le loro locazioni di memoria)

    Introduciamo anche un insieme \( Store \) delle memorie:
    \[ Store = Loc \farrow Val \]
    che ``estrae'' i valori dalle loro locazioni di memoria.

    Allo stesso modo in cui si possono concatenare ambienti, si possono concatenare anche memorie:
    \[ (S_1 S_2)(x) = \begin{cases}
        S_2(x) & \text{se } x\in dom(S_2) \\
        S_1(x) & \text{altrimenti}
    \end{cases}
\]
\end{cframe}


\begin{cdefframe}{DeepGreenLight}{DeepGreen}{Semantiche operazionali}{}
    \( Imp \) definisce due relazioni di valutazione: 
    \begin{itemize}
        \item una per le \textbf{espressioni} \( M \) (che producono valori senza avere side-effects):
            \[ \overset{M}{\leadsto} \subseteq  Env \times Exp \times Store \times Val \]
            \[ E \vdash M, S \overset{M}{\leadsto} v \]

        \item e una per i \textbf{programmi} \( p \) (che non producono valori ma cambiano la memoria)
            \[ \overset{p}{\leadsto} Env \times Imp \times Store \times Store \]
            \[ E \vdash p, S \overset{p}{\leadsto} S' \]
    \end{itemize}

    (ometteremo gli indici \( M \) e \( p \) perché deducibili dal contesto)
\end{cdefframe}


\begin{itemize}

    \item Costante:
        \begin{prooftree}
            \AxiomC{$E \vdash k, S \leadsto k$}
        \end{prooftree}

    \item Variabile:
        \begin{prooftree}
            \AxiomC{$E \vdash x, S \leadsto v \quad (se\ E(x)=l\ ed\ S(l)=v)$}
        \end{prooftree}

    \item Somma:
        \begin{prooftree}
            (se\ $v_1 + v_2 = v$)
            \AxiomC{$E \vdash M, S \leadsto v_1$}
            \AxiomC{$E \vdash N, S \leadsto v_2$}
            \BinaryInfC{$E \vdash M + N, S \leadsto v \quad $}
        \end{prooftree}

    \item Minore (vero):
        \begin{prooftree}
            (se\ $v_1 < v_2$)
            \AxiomC{$E \vdash M, S \leadsto v_1$}
            \AxiomC{$E \vdash N, S \leadsto v_2$}
            \BinaryInfC{$E \vdash M < N, S \leadsto true \quad $}
        \end{prooftree}

    \item Minore (falso):
        \begin{prooftree}
            (se\ $v_1 \ge v_2$) \ \
            \AxiomC{$E \vdash M, S \leadsto v_1$}
            \AxiomC{$E \vdash N, S \leadsto v_2$}
            \BinaryInfC{$E \vdash M < N, S \leadsto false \quad $}
        \end{prooftree}

    \item Skip:
        \begin{prooftree}
            \AxiomC{$E \vdash skip, S \leadsto S$}
        \end{prooftree}

    \item Sequenza:
        \begin{prooftree}
            \AxiomC{$E \vdash p, S \leadsto S'$}
            \AxiomC{$E \vdash q, S' \leadsto S''$}
            \BinaryInfC{$E \vdash p; q, S \leadsto S''$}
        \end{prooftree}

    \item If (condizione vera):
        \begin{prooftree}
            \AxiomC{$E \vdash M, S \leadsto true$}
            \AxiomC{$E \vdash p, S \leadsto S'$}
            \BinaryInfC{$E \vdash if\ M\ then\ p\ else\ q, S \leadsto S'$}
        \end{prooftree}

    \item If (condizione falsa):
        \begin{prooftree}
            \AxiomC{$E \vdash M, S \leadsto false$}
            \AxiomC{$E \vdash q, S \leadsto S'$}
            \BinaryInfC{$E \vdash if\ M\ then\ p\ else\ q, S \leadsto S'$}
        \end{prooftree}

    \item While (condizione vera):
        \begin{prooftree}
            \AxiomC{$E \vdash M, S \leadsto true$}
            \AxiomC{$E \vdash p, S \leadsto S'$}
            \AxiomC{$E \vdash while\ M\ do\ p, S' \leadsto S''$}
            \TrinaryInfC{$E \vdash while\ M\ do\ p, S \leadsto S''$}
        \end{prooftree}

    \item While (condizione falsa):
        \begin{prooftree}
            \AxiomC{$E \vdash M, S \leadsto false$}
            \UnaryInfC{$E \vdash while\ M\ do\ p, S \leadsto S$}
        \end{prooftree}

    \item Inizializzazione:
        \begin{prooftree}
            (con \( l \) nuova) \ \
            \AxiomC{$E \vdash M, S \leadsto v$}
            \AxiomC{$E(x,l)\vdash p, S(l,v) \leadsto S'$}
            \BinaryInfC{$E \vdash var\ x := M\ in\ p, S \leadsto S'$}
        \end{prooftree}

    \item Assegnamento:
        \begin{prooftree}
            se \( l = E(x) \) \ \ 
            \AxiomC{$E \vdash M, S \leadsto v$}
            \UnaryInfC{$E \vdash x := M, S \leadsto S(l,v)$} 
        \end{prooftree}

\end{itemize}

\begin{gframe}{}
    Notiamo che questa semantica non fa ``garbage collection'' !
\end{gframe}

\newpage

\section{\( All \): un linguaggio con procedure}
\( All \) rappresenta il nucleo di un linguaggio ``Algol-like''.

\( All \) estende \( Exp \in Imp \) con un costrutto sintattico per indicizzare array. Inoltre, distingue tra espressioni ``assegnabili'' (\( L-Exp \), ammesse alla sinistra di un assegnamento), e non.

\begin{defframe}{Grammatica del linguaggio \( All \)}{}
    \[ 
        k ::= 0 \mid 1 \mid \dots \mid true \mid false
    \]
    \[ 
        V (\in L-Exp) ::= x \mid x[M]
    \]
    \[
        M, N ::= k \mid V \mid M + N \mid M < N
    \]
    \vspace{-2em}
    \begin{align*}
        p, q ::= \ &skip \mid p;q \mid if \ M \ then \ p \ else \ q \mid while \ M \ do \ p \mid \\
                   &var \ x = M \ in \ p \mid arr \ x = [M_0, \dots, M_n] \  in \ p \mid V:= M \mid \\
                   &proc \ y(x) \ is \ p \ in \ q \mid call \ y(M)
    \end{align*}
    dove:
    \begin{itemize}
        \item \( V\) rappresenta quindi le \( L-Exp \)
        \item \( M, N \in Exp \)
        \item \( p, \ q \in Imp\) (programmi)
        \item \( arr \) dichiara un array \( x \) e gli assegna le espressioni \( M_0, \dots, M_n \) in \( p \)
        \item \( proc \) dichiara una procedura (funzione) \( y \) con parametro \( x \) in \( p \)
        \item \( call \) chiama la procedura \( y \) passandole come argomento \( M \)
    \end{itemize}
\end{defframe}

Gli array in \( All \) sono associati a sequenze finite e non vuote di locazioni (che indichiamo con \( Loc^+ \) {\small \color{gray}(con $^+$ ``chiusura positiva'' della stella di Kleene, ovvero insieme di stringhe di lunghezza finita sull'alfabeto, esclusa la stringa vuota)})

\begin{cframe}{DeepBlueLight}{DeepBlue}{Domini semantici}
    Definiamo \( Env \), l'insieme degli ambienti, in questo modo:
    \[ Env = Var \farrow Loc^+ \cup (Var \times All \times Env) \]
    {\small \color{gray}(array, variabile, corpo della funzione e ambiente in cui valutarla)}
\end{cframe}

\begin{cdefframe}{DeepGreenLight}{DeepGreen}{Semantiche operazionali}{}
    \begin{itemize}
        \item introduciamo una nuova relazione di valutazione per le \textbf{espressioni assegnabili}:
            \[
                \overset{V}{\leadsto} \subseteq Env \times L-Exp \times Store \times Loc
            \]
        \item oltre a quelle (già esistenti) per le \textbf{espressioni} \( M \):
            \[ \overset{M}{\leadsto} \subseteq Env \times Exp \times Store \times Val \]
            \[ E \vdash M, S \overset{M}{\leadsto} v \]

        \item e per i \textbf{programmi} \( p \):             
            \[ \overset{p}{\leadsto} \subseteq  Env \times Imp \times Store \times Store \]
            \[ E \vdash p, S \overset{p}{\leadsto} S' \]
    \end{itemize}
\end{cdefframe}

Oltre alle regole già definite in \( Imp \), il linguaggio \( All \) definisce:


\begin{itemize}
    \item Loc 1:
\begin{prooftree}
    \AxiomC{$E \vdash x, S \stackrel{l}{\leadsto} l$ \ \  se \( E(x) = l \)}
\end{prooftree}

\item Loc 2:
\begin{prooftree}
    \AxiomC{$E \vdash M, S \stackrel{M}{\leadsto} m$}
    \RightLabel{($\text{se } E(x) = \langle l_0, \ldots, l_n \rangle \text{ e } 0 \le m \le n$)}
    \UnaryInfC{$E \vdash x[M], S \stackrel{l}{\leadsto} l_m$}
\end{prooftree}

\item Ref:
\begin{prooftree}
    \AxiomC{$E \vdash V, S \stackrel{l}{\leadsto} l$}
    \RightLabel{($\text{se } S(l) = v$)}
    \UnaryInfC{$E \vdash V, S \stackrel{v}{\leadsto} v$}
\end{prooftree}

\item Assign:
\begin{prooftree}
    \AxiomC{$E \vdash M, S \stackrel{M}{\leadsto} v$}
    \AxiomC{$E \vdash V, S \stackrel{l}{\leadsto} l$}
    \BinaryInfC{$E \vdash V := M, S \stackrel{p}{\leadsto} S(l, v)$}
\end{prooftree}

\item Arr:
\begin{prooftree}
    \AxiomC{$E \vdash M_0, S \stackrel{v}{\leadsto} v_0$}
    \AxiomC{$\ldots$}
    \AxiomC{$E \vdash M_n, S \stackrel{v}{\leadsto} v_n$}
    \AxiomC{$E(x, \langle l_0, \ldots, l_n \rangle) \vdash p, S (l_i, v_i) \stackrel{v}{\leadsto} S'$}
    \RightLabel{($*$)}
    \QuaternaryInfC{$E \vdash \text{arr } x = [M_0, \ldots, M_n] \text{ in } p, S \stackrel{v}{\leadsto} S'$}
\end{prooftree}
\begin{center}
    $(*)$ dove $l_i$ è nuova, per $i = 0 \ldots n$
\end{center}

\item Proc:
\begin{prooftree}
    \AxiomC{$E(y, \langle x, p, E \rangle) \vdash q, S \stackrel{v}{\leadsto} S'$}
    \UnaryInfC{$E \vdash \text{proc } y(x) \text{ is } p \text{ in } q, S \stackrel{v}{\leadsto} S'$}
\end{prooftree}

\item Call by value:
\begin{prooftree}
    \AxiomC{$E \vdash M, S \stackrel{M}{\leadsto} v$}
    \AxiomC{$E'(x, l) \vdash p, S(l, v) \stackrel{v}{\leadsto} S'$}
    \RightLabel{($*$)}
    \BinaryInfC{$E \vdash \text{call } y(M), S \stackrel{v}{\leadsto} S'$}
\end{prooftree}
\begin{center}
    $(*)$ $\text{se } E(y) = \langle x, p, E' \rangle \text{ e } l \text{ è nuova}$
\end{center}

\end{itemize}

\chapter{Correttezza dei programmi}

\section{Correttezza nei linguaggi imperativi}

\subsection{Metodo delle invarianti}
La correttezza dei programmi viene spesso definita in realzione al contenuto di determinate variabili (per esempio, il contenuto finale di \texttt{result} è \texttt{x+y}). Questo risulta facile da tracciare se il programma consta di sole assegnazioni, ma più difficile se per esempio contiene loops.

Una possibile soluzione a questo problema è l'utilizzo delle \textbf{invarianti}.
\begin{defframe}{Invariante}{}
    Un predicato è detto \textbf{invariante} per una sequenza di operazioni quando il esso risulta vero prima e dopo l'esecuzione della sequenza. 
\end{defframe}

Una \textbf{loop invariant} è un'invariante (quindi una proprietà, un predicato) per cui si può dimostrare che, data per assunta la sua verità prima dell'inizio del loop, essa rimarrà vera al termine di un'iterazione. Se si può dimostrare che è vera prima di entrare nel loop e rimane vera al termine della prima iterazione, si può dedurre che rimarrà vera anche una volta terminato il loop.

{\color{gray}\small (Si tratta di una semplice dimostrazione per induzione: data \( P \) invariante, dobbiamo dimostrare \( Q(n) \) = ``\( P \) rimane vera dopo \( n \) iterazioni''. \( Q(1) \) è il caso base ed è vera per ipotesi; assumiamo per ipotesi induttiva che \( Q(n-1) \) valga. Visto che sappiamo che il contenuto di un'iterazione non cambia la verità di \( P \), possiamo dedurre che anche \( Q(n) \) valga (e quindi che \( Q \) valga \( \forall n \)))}

\subsubsection{Correttezza della moltiplicazione egizia}
Il papiro di Rhind (\~1650a.C.) descruve l'algoritmo usato dagli antichi egizi per svolgere la moltiplicazione. 

L'algoritmo della moltiplicazione egizia tra \( x \) e \( y \) in questo modo:
\begin{enumerate}
    \item Raddoppio \( x \).
    \item Se \( y \) è pari, lo divido per due; altrimenti, sottraggo uno e lo divido per due.
    \item Ripeto i passi (1) e (2) fino a ottenere \( y = 1 \)
    \item Mantengo solo le coppie in cui \( y \) è dispari, e sommo tra loro tutti i valori \( x \) di quelle coppie.
    \item Il risultato corrisponderà a \( x \times y \).
\end{enumerate}

Proviamo con \( 45 \times 138 \).
Moltiplico e divido per due seguendo l'algoritmo:
\begin{center}
    \begin{tabular}{r|l}
        45 & 138 \\
        90 & 69 \\
        180 & 34 \\
        360 & 17 \\
        720 & 8 \\
        1440 & 4 \\
        2880 & 2 \\
        5760 & 1 \\
    \end{tabular}
\end{center}

Mantengo solo le coppie con \( y \) dispari e sommo le \( x \) corrispondenti:
\begin{center}
    \begin{tabular}{r|l}
        90 & 69 \\
        360 & 17 \\
        5760 & 1 \\
    \end{tabular}
\end{center}

\[ 90 + 360 + 5760 = 6210 = 45 \times 138 \]

Definiamo l'algoritmo sotto forma di codice Java:

\begin{lstlisting}[language=Java]
public class Aegypt {
    public static void main(String[] args) {
        int a = 45;
        int b = 138;
        int x = a, y = b, res = 0;

        while (y >= 1) { 
            if (y % 2 == 0) {
                x = x + x;
                y = y/2;
            }
            else {
                res = res + x;
                y = y - 1;
            } 
        } 
        System.out.println(a + " times " + b + " is " + res);
    }
}
\end{lstlisting}

L'invariante di questo programma è:
\begin{center}
    \texttt{y>=0 \&\& a*b = x*y + res}
\end{center}

Infatti:
\begin{itemize}
    \item vale pre-loop: 
        \subitem abbiamo \texttt{x=a, y=b, res=0}, quindi (sostituendo) \texttt{a*b = a*b + 0} (e \texttt{y=b=45>=0})
    \item vale anche dopo una prima iterazione; abbiamo due casi:
        \begin{enumerate}
            \item \texttt{y\%2 == 0} - entro nell'\texttt{if}:
                \subitem \texttt{res} non cambia, e, se avevo \texttt{y>=0} prima, sicuramente anche \texttt{y/2 >= 0}
            \item \texttt{y\%2 != 0} - non entro nell'\texttt{if}:
                \begin{itemize}
                    \item il ``caso peggiore'' per \texttt{y} è \texttt{y = 1} (ultima iterazione del loop) - anche in quel caso, \texttt{y = 1 - 1 = 0}, e la proprietà \texttt{y>=0} rimane vera.
                    \item per \texttt{a*b = x*y + res}, invece: avendo \texttt{y-=1}, stiamo moltiplicando \texttt{x} per un valore più piccolo di 1 (quindi stiamo essenzialmente sottraendo una \texttt{x}); questa sottrazione viene però compensata da \texttt{res += x} (quello che viene tolto da \texttt{x*y} viene riaggiunto a \texttt{res}).

                \end{itemize}
        \end{enumerate}

\end{itemize}

Abbiamo quindi che \texttt{y>=0 \&\& a*b = x*y + res} vale prima del loop, e dopo una qualsiasi iterazione del loop. Possiamo quindi concludere che è un'invariante per questo programma !

\section{Logica di Hoare}
(introdotta nel 1969 da C.A.R. Hoare (inventore del quicksort !))

La \textbf{logica di Hoare} è un sistema formale che rientra tra le semantiche assiomatiche e permette di valutare la correttezza di programmi utilizzando formalismi matematici.

\begin{defframe}{Semantica della logica di Hoare}{}
    \[
        M, N ::= k \mid x \mid M + N
    \]  
    \[
        A, B ::= true \mid false \mid A \supset B \mid M < N \mid M = N
    \]    
    \[
        p, q ::= skip \mid p;q \mid x := M \supset N \mid if \ B \ then \ p \ else \ q \mid while \ B \ do p 
    \]

    dove:
    \begin{itemize}
        \item \( M, N \) sono \textbf{espressioni numeriche}
        \item \( A, B \) sono \textbf{espressioni booleane}
        \item \( p, q\) sono \textbf{programmi}
        \item introduciamo il simbolo \( \supset \) per indicare l'\textbf{implicazione logica} \( \implies \) (Peano–Russell notation)
    \end{itemize}
    (usiamo una versione minimalista delle operazioni booleane - gli altri simboli si possono derivare (per esempio, \( \neg A \equiv A \supset false \)))
\end{defframe}

\begin{defframe}{Tripla di Hoare}{}
    Siano \( A \) e \( B \) due espressioni booleane e \( p \) un programma.\\
    La tripla:
    \[ \{A\} \ p \ \{B\} \]

    significa ``se \( A \) è soddisfatto prima dell'esecuzione di \( p \), allora \( B \) è soddisfatto dopo la terminazione di \( p \), se questa avviene.''

    {\small \color{gray}(essenzialmente, se eseguo \( p \) in uno ``stato'' che soddisfa \( A \), ottengo uno ``stato'' che soddisfa \( B \).)}

    \( A \) viene chiamata \textit{precondizione}, e \( B \) \textit{postcondizione}.

    \begin{gframe}{}
        \begin{itemize}
            \item nota bene ! questa interpretazione della tripla di Hoare si chiama \textbf{correttezza parziale}: la postcondizione vale infatti \textbf{a condizione che \textit{p} termini}.
        \end{itemize}
    \end{gframe}

Quindi, definiamo come \textbf{formula} un'espressione che appartiene alla grammatica:
\[ \varphi ::= \{A\} \ p \ \{B\} \]

\end{defframe}

\begin{cdefframe}{DeepGreenLight}{DeepGreen}{Regole di inferenza generali}
\begin{itemize}
    \item regola del \textbf{true}:
\[
\AxiomC{}
\RightLabel{$(true)$}
\UnaryInfC{$\{P\} \ C \ \{true\}$}
\DisplayProof
\]

\item regola del \textbf{false}:
\[
\AxiomC{}
\RightLabel{$(false)$}
\UnaryInfC{$\{false\} \ C \ \{P\}$}
\DisplayProof
\]

    \item regola dello \textbf{strengthening}, ovvero rafforzamento della precondizione:

\[
\AxiomC{$P \supset Q$}
\AxiomC{$\{Q\} \ C \ \{R\}$}
\RightLabel{$(str)$}
\BinaryInfC{$\{P\} \ C \ \{R\}$}
\DisplayProof
\]

    \item regola dello \textbf{weakening}, ovvero indebolimento della postcondizione:
\[
\AxiomC{$\{P\} \ C \ \{Q\}$}
\AxiomC{$Q \supset R$}
\RightLabel{$(weak)$}
\BinaryInfC{$\{P\} \ C \ \{R\}$}
\DisplayProof
\]
    \item regola dell'\textbf{and}:

\[
\AxiomC{$\{P\} \ C \ \{Q_0\}$}
\AxiomC{$\dots$}
\AxiomC{$\{P\} \ C \ \{Q_n\}$}
\RightLabel{$(and)$}
\TrinaryInfC{$\{P\} \ C \ \{Q_0 \wedge \dots \wedge Q_n\}$}
\DisplayProof
\]

    \item regola dell'\textbf{or}:
\[
\AxiomC{$\{P_0\} \ C \ \{Q\}$}
\AxiomC{$\dots$}
\AxiomC{$\{P_n\} \ C \ \{Q\}$}
\RightLabel{$(or)$}
\TrinaryInfC{$\{P_0 \vee \dots \vee P_n\} \ C \ \{Q\}$}
\DisplayProof
\]


\end{itemize}
\end{cdefframe}


\begin{cdefframe}{BrightGreenLight}{BrightGreen}{Regole di inferenza per i programmi}
\begin{itemize}
    \item regola dello \textbf{skip}:
\[
\AxiomC{}
\RightLabel{$(skip)$}
\UnaryInfC{$\{P\} \ skip \ \{P\}$}
\DisplayProof
\]


    \item regola dell'\textbf{assign}:
\[
\AxiomC{}
\RightLabel{$(assign)$}
\UnaryInfC{$\{[E/x]P\} \ x := E \ \{P\}$}
\DisplayProof
\]

    \item regola dell'\textbf{if-then-else}:

\[
\AxiomC{$\{P \wedge Q\} \ C_1 \ \{R\}$}
\AxiomC{$\{P \wedge \neg Q\} \ C_2 \ \{R\}$}
\RightLabel{$(if)$}
\BinaryInfC{$\{P\} \ if \ Q \ then \ C_1 \ else \ C_2 \ \{R\}$}
\DisplayProof
\]

    \item regola del \textbf{while}:
\[
\AxiomC{$\{P \wedge Q\} \ C \ \{P\}$}
\RightLabel{$(while)$}
\UnaryInfC{$\{P\} \ while \ Q \ do \ C \ \{P \wedge \neg Q\}$}
\DisplayProof
\]

    \item regola della \textbf{concatenazione}:

\[
\AxiomC{$\{P\} \ C_1 \ \{Q\}$}
\AxiomC{$\{Q\} \ C_2 \ \{R\}$}
\RightLabel{$(comp)$}
\BinaryInfC{$\{P\} \ C_1 ; C_2 \ \{R\}$}
\DisplayProof
\]
\end{itemize}

\end{cdefframe}

\subsection{Correttezza della divisione intera}

Il seguente programma calcola la divisione intera di \( x \div y \).

\begin{lstlisting}[language=C]
b := x;
a := 0;
while (b >= y) do
    b = b-y;
    a = a+1;
\end{lstlisting}
(il resto si trova in \( b \))

Vogliamo trovare la \textbf{tripla di Hoare} per questo programma, e dimostrare la sua correttezza.

Notiamo che una tripla adatta sarebbe:
\[ \{x \geq 0\} \ p \ \{ay+b=x \ \land \ b\geq 0 \ \land b < y\} \]

(Analizziamola:
\begin{itemize}
    \item il programma va in loop quando \( y \leq 0 \leq x \), ma controllare \( y\geq 0 \) non ci interessa perché ne stiamo dimostrando la \textit{correttezza parziale}. Ci basta quindi \( x \geq 0 \), per assicurarci che si entri nel \( while \).
    \item \( ay + b = x \) indica la corretta esecuzione della divisione
    \item \( b \geq 0 \) è necessario, in quanto il resto non può essere negativo (non possiamo dividere una o più volte di troppo)
    \item \( b < y \) ci assicura che la divisione sia finita (che non avremmo potuto dividere una volta in più)
\end{itemize})

\begin{gframe}{}
    Notiamo che anche un programma come 
    \begin{lstlisting}[language=C]
        a := 0;
        b := 0;
        y := 1;
        x := 0;
    \end{lstlisting}
    rispetterebbe la tripla di Hoare (senza però eseguire la divisione intera tra \( x \) e \( y \)).
    
    Per impedire una situazione come questa dovremmo usare il simbolo \( ' \) per riferirci, nella postcondizione, ai valori che le variabili avevano ad inizio programma. 
    \\ La tripla diventerebbe quindi \( ay'+b=x' \land b\geq 0 \land b<y \).
    (Alternativamente, potremmo far sì che i programmi non possano modificare i valori di input).

    In ogni caso, prenderemo come buona la specifica senza apici.
\end{gframe}

Passiamo ora a dimostrare la correttezza della tripla tramite le regole di inferenza della logica di Hoare.

Dividiamo il programma in sezioni per semplificarne la dimostrazione.

\begin{itemize}
    \item \texttt{b := x;}
\end{itemize}

{\color{gray}

Dobbiamo avere \( b = x \land b\geq 0 \) (per la precondizione della tripla di Hoare).

\begin{prooftree}
\AxiomC{??}
\AxiomC{\( b := x \)}
\AxiomC{\( \{b = x \land b \geq 0\} \)}
\TrinaryInfC{\( \{x \geq 0\} \ b := x \ \{b = x \land b>= 0\}\)}
\end{prooftree}

Qual è la più debole precondizione che, dato \( b := x \), ci permette di affermare che sicuramente \( \{ b=x \land b\geq 0 \}\) sarà soddisfatta?
}

La weakest precondition ci viene data dalla regola dell'\textbf{assign}, \( \{[\sfrac{M}{x}]A\} \ x:= M \ \{A\}\).
{\color{gray}\small (la precondizione è la postcondizione, in cui si sostituisce \( x \) con \( 
M \))}

\begin{prooftree}
    \AxiomC{\( \{ x = x \land x\geq 0 \}\)}
\AxiomC{\( b := x \)}
\AxiomC{\( \{b = x \land b \geq 0\} \)}
\TrinaryInfC{\( \{x \geq 0\} \ b := x \ \{b = x \land b>= 0\}\)}
\end{prooftree}

Notiamo però che la precondizione della regola dell'assign non corrisponde a quella della tripla di Hoare che stiamo cercando di dimostrare. Ci serve un altro passaggio per determinare la sua correttezza. Usiamo la regola dello \textbf{strengthening} per arrivare dalla premessa che vogliamo dimostrare a quella che abbiamo.

\begin{prooftree}
    \AxiomC{\( (x\geq 0) \supset (x = x \land x\geq 0) \)}
    \AxiomC{\( \{ x = x \land x\geq 0 \} \ \ b := x \ \  \{b = x \land b \geq 0\} \)}
    \BinaryInfC{\( \{x \geq 0\} \ b := x \ \{b = x \land b \geq  0\}\)}
\end{prooftree}

Non ci serve dimostrare la correttezza dell'implicazione \( (x\geq 0) \supset (x = x \land x\geq 0) \), in quanto presumiamo che nella nostra logica le verità dell'aritmetica (come \( x = x \)) e i teoremi della logica classica (come \( a \land b \supset a \)) siano già dimostrati.

Ci interessa ora continuare la composizione della dimostrazione passando alla prossima sezione del programma, e utilizzando la post-condizione appena dimostrata come pre-condizione.

\begin{itemize}
    \item \texttt{a := 0}
\end{itemize}

Usiamo lo stesso ragionamento visto sopra (assign + strengthening).

\begin{prooftree}
    \AxiomC{\( (b = x \land b \geq 0) \supset (0y + b = y \land b\geq 0) \)}
    \AxiomC{\( \{ 0y + b = x \land b>= 0  \} \ \ a := 0 \ \ \{ ay + b = x \land b \geq 0 \} \)}
    \BinaryInfC{\( \{b = x \land b \geq 0\} \ a := 0 \ \{ay + b = x \land b \geq  0\}\)}
\end{prooftree}

\begin{itemize}
    \item \texttt{b := x; a := 0;}
\end{itemize}

Ora dobbiamo mettere insieme le due dimostrazioni. Visto che la pre-condizione della seconda corrisponde alla post-condizione della prima, possiamo usare la regola della \textbf{concatenazione}:

\begin{prooftree}
    \AxiomC{\( \{x \geq 0\} \ b := x \ \{b = x \land b\geq  0\}\)}
    \AxiomC{\( \{b = x \land b \geq 0\} \ a := 0 \ \{ay + b = x \land b >= 0\}\)}
    \BinaryInfC{\( \{x \geq 0\} \ b := x ; \ a := 0 \ \{ay + b = x \land b >= 0\}\)}
\end{prooftree}

Sappiamo quindi che, prima di entrare nel \( while \), la condizione \( \{ay + b = x \land b >= 0\} \), che da ora in poi chiameremo \( A \), è soddisfatta.

\begin{itemize}
    \item \texttt{while (b >= y) do b := b-y; a := a+1;}
\end{itemize}

Dobbiamo quindi applicare la regola del \textbf{while} 
{\color{gray}
    \(\left(
\AxiomC{$\{P \wedge Q\} \ C \ \{P\}$}
\RightLabel{$(while)$}
\UnaryInfC{$\{P\} \ while \ Q \ do \ C \ \{P \wedge \neg Q\}$}
\DisplayProof
\right)
\)
}

La situazione è quindi:

\begin{prooftree}
    \AxiomC{\( \{A \land b \geq y\} \ \ b:= b-y; \ a:= a+1; \ \ \{A\} \)}
    \UnaryInfC{\( \{ay + b = x \land b \geq 0\}  \ \ while \ (b \geq y) \ do \ b:= b-y; \ a:=a+1 \ \ \{A \land \neg (b < y)\}\)}
\end{prooftree}

Per sviluppare la dimostrazione di \( \{A \land b \geq y\} \ \ b:= b-y; \ a:= a+1; \ \ \{A\} \), possiamo usare una strategia analoga a quella vista sopra.

Dobbiamo trovare una ``condizione ponte'' per poter utilizzare la regola della concatenazione. Ci conviene partire dalla destra e applicare la regola dell'assegnamento per trovare la weakest precondition.

Otteniamo quindi: 
\[ \{(a+1)y + b = x \land b>= 0 \land b < y\}  \ \ a:= a+1; \ \ \{A\} \]

Usiamo la pre-condizione appena trovata come post-condizione per applicare la regola dell'assign su \( b := b-y; \).
\[ \{(a+1)y + (b-y) = x \land b-y>= 0 \land b < y\}  \ \ b:= b-y; \ \ \{(a+1)y + b = x \land b\geq 0\} \]

Abbiamo in questo modo trovato una condizione che faccia da ``ponte'' tra i due assegnamenti e ci permetta di utilizzare la regola della concatenazione. 

Ma notiamo che, anche in questo caso, la nostra premessa \( \{(a+1)y + (b-y) = x \land b-y>= 0 \land b < y\}  \) non coincide con \( A \land b\geq y\), pre-condizione da cui partiamo per la regola del while. Applichiamo quindi anche in questo caso lo strengthening:
\[ \{A \land b\geq y\} \supset \{(a+1)y + (b-y) = x \land b-y \geq 0\} \]
{\color{gray}(ovvero \((ay + b = x \land b \geq y) \supset (ay + b = x \land b-y \geq 0) \), evidentemente vero nell'aritmetica). }

\vspace{1em}
(Il tutto, in un unico albero di inferenza:
{\tiny
\begin{prooftree}
  \AxiomC{\(\{(a+1)y + b = x \land b \ge 0\}\; a := a+1\; \{ay + b = x \land b \ge 0\}\)}
  \UnaryInfC{\(\{(a+1)y + b = x \land b \ge 0\}\; a := a+1\; \{A\}\)}

  \AxiomC{\(\{(a+1)y + (b - y) = x \land (b - y) \ge 0\}\; b := b - y\; \{(a+1)y + b = x \land b \ge 0\}\)}
  \UnaryInfC{\(\{(a+1)y + (b - y) = x \land b - y \ge 0\}\; b := b - y\; \{(a+1)y + b = x \land b \ge 0\}\)}

  \BinaryInfC{\(\{(a+1)y + (b - y) = x \land b - y \ge 0\}\;
                 b := b - y;\; a := a+1\;
                 \{A\}\)}

  \UnaryInfC{\(\{ay + b = x \land b \ge y\}\;
                 b := b - y;\; a := a+1\;
                 \{A\}\)}

  \UnaryInfC{\(\{A\}\; while \ (b \ge y) \ do \ \ b := b - y;\; a := a+1\; \{A \land \neg(b \ge y)\}\)}
\end{prooftree}
}
)
\section{Correttezza nei linguaggi funzionali}
Per dimostrare la correttezza dei programmi funzionali, adottiamo una \textbf{logica equazionale}.

Prima di definire le regole di questa logica, ci è utile re-introdurre una proposizione trattata all'inizio del corso (``omomorfismo tra algebre con la stessa segnatura'', p.\pageref{teo-om}). 

\begin{propframe}{Omomorfismi e numeri naturali}{}
    \label{prop1}
    Dati un insieme \( A \), un elemento \( a \in A \) ed una funzione \( h: A\to A \), esiste \textbf{una sola} \( f:\N \to A \) (omomorfismo) tale che \( f(0)=a \) e \( f(succ \ n) = h(f(n))\)
\end{propframe}

\begin{gframe}{}
Notiamo che l'algebra fornita non deve essere necessariamente induttiva (è \( \N \) ad esserlo).
\end{gframe}

Prendiamo per esempio \( Bool \), l'algebra (non induttiva) dei booleani (definita a p. \pageref{Bool})

Le due algebre \( \N \) e \( Bool \) hanno la stessa segnatura. Per il teorema, esiste quindi un unico omomorfismo (chiamiamolo \( f \)) tra di loro, tale che:
\begin{itemize}
    \item \( f(zero) = true \)
    \item \( f(succ \ x ) = not(f(x)) \)
\end{itemize}

%\[
%\begin{tikzcd}
%& 1 \arrow[dl] \arrow[dr] & \\
%Nat \arrow[loop left, "Succ"] \arrow[rr, "f"] & & Bool \arrow[loop right, "not"]
%\end{tikzcd}
%\]
\begin{centering}
    
\begin{tikzpicture}[
    node distance=3.5cm,
    auto,
    >=Latex,
    thick,
    scale=1.3,
    every node/.style={transform shape}
]
  \node[circle, draw, thick, minimum size=1.2cm] (Nat) {Nat};
  \node[circle, draw, thick, minimum size=1.2cm, right of=Nat] (Bool) {Bool};
  \node[above right=1.2cm and 1cm of Nat] (One) {\( \mathbb{1} \)};

  \draw[->] (Nat) to node[below] {f} (Bool);
  \draw[->] (One) -- node[left] {zero} (Nat);
  \draw[->] (One) -- node[right] {true} (Bool);
  \draw[->] (Nat) to[loop left] node[left] {succ} ();
  \draw[->] (Bool) to[loop right] node[right] {not} ();
\end{tikzpicture}

\end{centering}

Notiamo che questa funzione corrisponde perfettamente alla funzione \texttt{is\_even} precedentemente definita in SML in questo modo:
\begin{lstlisting}
    fun is_even 0 = true
        | is_even(succ n) = not(is_even n);
\end{lstlisting}

\texttt{is\_even} è quindi l'unico omomorfismo tra \( \N \) e \(Bool\).

\begin{defframe}{Operatore \( \rho \)}{}
Indichiamo con \( \rho \) la funzione che, dati \( a \) e \( h \) (``costruttori'') come nella proposizione 1,  ci restituisce l'omorfismo \( f \). 

Si ha dunque:
\begin{itemize}
    \item \( f = \rho \ a \ h \)
    \item \( \rho \ a \ h \ zero = a \)
    \item \( \rho \ a \ h \ (succ \ n) = h(\rho \ a \ h \ n) \).
\end{itemize}

        Applicando ricorsivamente queste equazioni, si ottiene:
        \[ \rho a h n = \rho a h \underbrace{(succ(\ldots(succ\ 0)\ldots))}_{n \text{ volte}} = h(\ldots(h\ a)\ldots) \]
        
Ovvero, \(  \rho a h n \) itera \( n \) volte \( h \) a partire da \( a \). Viene per questo chiamato \textit{iteratore}.
\begin{gframe}{}
Possiamo anche usare \( \rho \) per definire i numeri di Church:
\[ c_n \ h \ a = \rho \ a \ h \ n \]
{\color{gray}(visto che un numero di Church è definito come un applicare \( n \) volte \( h \) a partire da \( a \))}
\end{gframe}
\end{defframe}

{\color{CadetBlue}\small Quindi, essenzialmente, \( \rho \) prende gli equivalenti di ``zero'' e ``succ'' e restituisce l'omomorfismo da \( \N \) all'insieme che è definito dai costruttori forniti.}

\begin{centering}
Si ha quindi questa situazione:

\begin{tikzpicture}[
    node distance=3.5cm,
    auto,
    >=Latex,
    thick,
    scale=1.3,
    every node/.style={transform shape}
]
  \node[circle, draw, thick, minimum size=1.2cm] (Nat) {Nat};
  \node[circle, draw, thick, minimum size=1.2cm, right of=Nat] (A) {A};
  \node[above right=1.2cm and 1cm of Nat] (One) {\( \mathbb{1} \)};

  \draw[->] (Nat) to node[below] {\( \rho \ a \ h \)} (A);
  \draw[->] (One) -- node[left] {zero} (Nat);
  \draw[->] (One) -- node[right] {\( a \)} (A);
  \draw[->] (Nat) to[loop left] node[left] {succ} ();
  \draw[->] (A) to[loop right] node[right] {\( h \)} ();
\end{tikzpicture}

\end{centering}

\vspace{1em}
Definiamo quindi un primo linguaggio funzionale che contenga quanto introdotto.

\begin{defframe}{\( Fun_{\rho} \)}{}
    La grammatica di questo linguaggio è:
        \[
        M, N ::= zero \mid x \mid fn \ x \implies M \mid MN \mid succ(M) \mid \rho(M, N)
    \]

    e deve essere vero che:
    \begin{itemize}
        \item \( \rho(M, N)zero = M \) 
            \subitem {\color{gray} (l'omomorfismo, applicato su \( zero \), deve portare al costruttore unario dell'altra algebra, ovvero \( M \) (come l'elemento neutro))}
        \item \( \rho(M, N)(succ \ L) = N\bigl(\rho(M, N)L\bigr) \)  
            \subitem {\color{gray} (l'omomorfismo, applicato su \( succ \ L \), deve portare all'operazione di \( A \) applicata su \( L \), ma nell'altro insieme, ovvero \( \rho(M, N)L \))}

    \end{itemize}
\end{defframe}

Possiamo, per esempio, usare i costrutti di questo linguaggio per definire \( is\_even \) tramite \( \rho \).
\[ is\_even \equiv \rho \ true \ not \]
Notiamo però che questo linguaggio non è ancora perfetto.

Consideriamo per esempio l'algebra induttiva delle liste finite di numeri naturali definita dai costruttori \( empty: \mathbb{1}\to N-List \) \ e \ \( cons: (N-List \times \N) \to N-List \) (definita a come a p.\pageref{alist}).

Possiamo definirla in SML in questo modo:
\begin{lstlisting}
    datatype N-list = empty of Unit 
        | cons of (N-list * N);
\end{lstlisting}

Introduciamo anche una funzione \texttt{nicelist}:
\begin{lstlisting}
    fun nicelist(0) = empty()
        | nicelist(succ n) = cons(nicelist n, succ n);
\end{lstlisting}

\begin{gframe}{}
    Per capire meglio come funzionino i costruttori ricorsivi, ci può essere utile simulare il costruttore \texttt{cons}.

    Vediamo cosa succede se costruiamo \texttt{nicelist(succ 2)}

    \texttt{nicelist(succ 2) = cons(nicelist 2, succ 2)}

\(
\begin{array}{rcl}
\langle 3, & \underbrace{ \phantom{0000000000} }_{\text{\footnotesize nicelist 2}} \ \rangle & = \langle 3,2,1 \rangle \\[1em]
\langle 2, & \underbrace{ \phantom{000000} }_{\text{\footnotesize nicelist 1}} \ \rangle & \\[1em]
\langle 1, & \underbrace{ \phantom{0000} }_{\text{\footnotesize nicelist 0 = \(\emptyset\)}} \rangle & \\[1em]
\end{array}
\)
\end{gframe}

Contrariamente a quanto avveniva prima, per calcolare \( nicelist(succ \ n) \) non ci basta \( nicelist(n) \), ma ci serve anche \( n \) stesso (l'equivalente di \( succ \) non è più una endofunzione, prende anche da un altro dominio).

Dobbiamo quindi introdurre una nuova versione della \hyperref[prop1]{Proposizione 1}.

\begin{propframe}{Omomorfismi e numeri naturali (v. 2)}{}
    Dati un insieme \( A \), un elemento \( a \in A \) e una funzione \( h: A \to (\N \to A) \), esiste una sola funzione \( f: \N \to A \) tale che \( f(0) = a \) \ e \ \( f(succ \ n) = h(f(n), n) \).
\end{propframe}

Definiamo quindi un equivalente di \( \rho \) che restituisca questo omomorfismo. Lo chiamiamo \( rec \).

\begin{defframe}{Operatore \( rec \)}{}
    L'operatore \( rec \), dato un elemento \( a \in A \) e una funzione \( A \to (\N \to A) \), restituisce l'unico omomorfismo \( \N \to A \).
\end{defframe}

L'operatore \( rec \) appartiene al \textit{System-T} di Gödel.

Introduciamo quindi un linguaggio funzionale che contenga \( rec \).

\begin{defframe}{\( Fun_{rec} \)}{}
    La grammatica di questo linguaggio è:
    \[
        M, N ::= zero \mid x \mid fn \ x \implies M \mid MN \mid succ(M) \mid rec(M, N)
    \]
    dove:
    \begin{itemize}
        \item \( rec \ MN \ zero = M \)
        \item \( rec \ MN \ succ(L) = N((rec \ M,N)L)L \) \quad {\color{gray}\( h(f(n), n) \)}
    \end{itemize}
\end{defframe}

Possiamo usare \( rec \) anche per fare tutto ciò che faceva \( \rho \) (ci basta ``ignorare'' il secondo argomento della funzione \( N\) fornita). 

Per esempio, la funzione \( is\_even \) si può definire tramite \( rec \) in questo modo:
\[ is\_even \equiv rec(true, fn \ y z \implies not \ y) \]
{\color{gray}(\( rec \) vuole una funzione a due argomenti, ma noi ne utilizziamo semplicemente uno solo (ignoriamo \( z \)))}

Possiamo definire anche altre funzioni a noi note tramite \( rec \):
\[ fact \equiv rec(1, fn \ yz \implies y * (succ \ z)) \]
\[ plus \equiv fn \ x y \implies rec(y, fn \ w \ z \implies succ \ w) x\]

\begin{gframe}{}
Proviamo a calcolare $plus(2, 3)$, dove $x=2$ e $y=3$.

Sia $R = rec(3,\ fn\ w\ z \Rightarrow succ(w))$ (l'omomorfismo). Dunque $plus(x, y) = R(x)$.

Abbiamo \( R(x) = \begin{cases}
    y & \text{se } x = zero \\
    succ(R(L)) & \text{se } x = succ(L) \\
\end{cases} \)

Simuliamo quindi una possibile esecuzione:

\begin{displaymath}
\begin{array}{rll}
plus(2, 3) & \equiv R(2) \\
& \equiv R(succ(1)) & 2 = succ(1) \\
& \equiv succ(R(1)) & x = 2 = succ(1) \text{ quindi } R(x) = succ(R(1))\\
& \equiv succ(R(succ(0))) & 1 = succ(0) \\
& \equiv succ(succ(R(0))) & \text{stesso passo ricorsivo }(succ(R(0))) \\
& \equiv succ(succ(3)) & \text{caso base \( M \): } rec\ M\ N\ zero = M = 3. \\
& \equiv succ(4) & \\
& \equiv 5 & \\
\end{array}
\end{displaymath}
\end{gframe}

\end{document}
